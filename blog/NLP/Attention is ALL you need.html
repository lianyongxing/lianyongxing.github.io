<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Attention is ALL you need | LIANYONGXING. BLOG</title>
    <meta name="description" content="May the force be with u.">
    <meta name="generator" content="VuePress 1.4.0">
    <link rel="icon" href="ficon.jpeg">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/2.10.0/github-markdown.min.css">
    
    <link rel="preload" href="/assets/css/0.styles.a5323663.css" as="style"><link rel="preload" href="/assets/js/app.acaacfed.js" as="script"><link rel="preload" href="/assets/js/2.858fe96a.js" as="script"><link rel="preload" href="/assets/js/39.384a454e.js" as="script"><link rel="prefetch" href="/assets/js/10.652038c7.js"><link rel="prefetch" href="/assets/js/100.2dd9d0f2.js"><link rel="prefetch" href="/assets/js/101.487a243a.js"><link rel="prefetch" href="/assets/js/102.0c61121f.js"><link rel="prefetch" href="/assets/js/103.4dbe63e3.js"><link rel="prefetch" href="/assets/js/104.7f502b03.js"><link rel="prefetch" href="/assets/js/105.6e83f9e6.js"><link rel="prefetch" href="/assets/js/106.5409dca1.js"><link rel="prefetch" href="/assets/js/107.7ad54acd.js"><link rel="prefetch" href="/assets/js/108.3486f36a.js"><link rel="prefetch" href="/assets/js/109.5ee4cd3b.js"><link rel="prefetch" href="/assets/js/11.4f8ea0df.js"><link rel="prefetch" href="/assets/js/110.6417250e.js"><link rel="prefetch" href="/assets/js/111.ded28c89.js"><link rel="prefetch" href="/assets/js/112.94f5b6c6.js"><link rel="prefetch" href="/assets/js/113.3b1fe42e.js"><link rel="prefetch" href="/assets/js/114.0e643f0c.js"><link rel="prefetch" href="/assets/js/115.32a16f28.js"><link rel="prefetch" href="/assets/js/12.a9ef9aaf.js"><link rel="prefetch" href="/assets/js/13.acff38a0.js"><link rel="prefetch" href="/assets/js/14.113e66b1.js"><link rel="prefetch" href="/assets/js/15.19b59d69.js"><link rel="prefetch" href="/assets/js/16.4ee77b14.js"><link rel="prefetch" href="/assets/js/17.a5b6d566.js"><link rel="prefetch" href="/assets/js/18.744a9950.js"><link rel="prefetch" href="/assets/js/19.fd9e7099.js"><link rel="prefetch" href="/assets/js/20.66903b5c.js"><link rel="prefetch" href="/assets/js/21.c98f5d06.js"><link rel="prefetch" href="/assets/js/22.a0056c91.js"><link rel="prefetch" href="/assets/js/23.73369199.js"><link rel="prefetch" href="/assets/js/24.dc5b3fc5.js"><link rel="prefetch" href="/assets/js/25.bf612edb.js"><link rel="prefetch" href="/assets/js/26.b9dd7686.js"><link rel="prefetch" href="/assets/js/27.371379cd.js"><link rel="prefetch" href="/assets/js/28.4a33fe83.js"><link rel="prefetch" href="/assets/js/29.be42af3d.js"><link rel="prefetch" href="/assets/js/3.47952932.js"><link rel="prefetch" href="/assets/js/30.37bd8e9e.js"><link rel="prefetch" href="/assets/js/31.58f89f44.js"><link rel="prefetch" href="/assets/js/32.741f73e3.js"><link rel="prefetch" href="/assets/js/33.a9c7dcb5.js"><link rel="prefetch" href="/assets/js/34.c3f829b5.js"><link rel="prefetch" href="/assets/js/35.29c897d0.js"><link rel="prefetch" href="/assets/js/36.1e02f622.js"><link rel="prefetch" href="/assets/js/37.a1d23a14.js"><link rel="prefetch" href="/assets/js/38.06964860.js"><link rel="prefetch" href="/assets/js/4.7feb14c8.js"><link rel="prefetch" href="/assets/js/40.c7e4dfb7.js"><link rel="prefetch" href="/assets/js/41.3e5c5125.js"><link rel="prefetch" href="/assets/js/42.dcf19498.js"><link rel="prefetch" href="/assets/js/43.54203bcb.js"><link rel="prefetch" href="/assets/js/44.9cccb36c.js"><link rel="prefetch" href="/assets/js/45.3ce2a56c.js"><link rel="prefetch" href="/assets/js/46.17aac222.js"><link rel="prefetch" href="/assets/js/47.57ed0257.js"><link rel="prefetch" href="/assets/js/48.8baebce7.js"><link rel="prefetch" href="/assets/js/49.7eb16be5.js"><link rel="prefetch" href="/assets/js/5.55c31470.js"><link rel="prefetch" href="/assets/js/50.06274b55.js"><link rel="prefetch" href="/assets/js/51.b50e24a8.js"><link rel="prefetch" href="/assets/js/52.7b92239f.js"><link rel="prefetch" href="/assets/js/53.fb7cbb6a.js"><link rel="prefetch" href="/assets/js/54.d047b5f1.js"><link rel="prefetch" href="/assets/js/55.6e2fd120.js"><link rel="prefetch" href="/assets/js/56.fa7ee707.js"><link rel="prefetch" href="/assets/js/57.31b7d0a7.js"><link rel="prefetch" href="/assets/js/58.80cddbc7.js"><link rel="prefetch" href="/assets/js/59.92b2b0f2.js"><link rel="prefetch" href="/assets/js/6.792ce084.js"><link rel="prefetch" href="/assets/js/60.9ab8e458.js"><link rel="prefetch" href="/assets/js/61.b3abbf87.js"><link rel="prefetch" href="/assets/js/62.2bde1ed2.js"><link rel="prefetch" href="/assets/js/63.39cf6423.js"><link rel="prefetch" href="/assets/js/64.c839079e.js"><link rel="prefetch" href="/assets/js/65.ea1754c1.js"><link rel="prefetch" href="/assets/js/66.802ec528.js"><link rel="prefetch" href="/assets/js/67.a30d6fff.js"><link rel="prefetch" href="/assets/js/68.d98ac090.js"><link rel="prefetch" href="/assets/js/69.083b1217.js"><link rel="prefetch" href="/assets/js/7.3964fa52.js"><link rel="prefetch" href="/assets/js/70.272720ba.js"><link rel="prefetch" href="/assets/js/71.386b54d7.js"><link rel="prefetch" href="/assets/js/72.005e615f.js"><link rel="prefetch" href="/assets/js/73.b1266355.js"><link rel="prefetch" href="/assets/js/74.9ab08f58.js"><link rel="prefetch" href="/assets/js/75.eefe4ff0.js"><link rel="prefetch" href="/assets/js/76.e4162716.js"><link rel="prefetch" href="/assets/js/77.e1ead4d7.js"><link rel="prefetch" href="/assets/js/78.ae9b9cc3.js"><link rel="prefetch" href="/assets/js/79.eeaf4627.js"><link rel="prefetch" href="/assets/js/8.5ccbfc45.js"><link rel="prefetch" href="/assets/js/80.0272858c.js"><link rel="prefetch" href="/assets/js/81.bbc0907f.js"><link rel="prefetch" href="/assets/js/82.4de7fcc4.js"><link rel="prefetch" href="/assets/js/83.2ad342e1.js"><link rel="prefetch" href="/assets/js/84.3398b3bb.js"><link rel="prefetch" href="/assets/js/85.be446d7c.js"><link rel="prefetch" href="/assets/js/86.1ef2094d.js"><link rel="prefetch" href="/assets/js/87.53795a29.js"><link rel="prefetch" href="/assets/js/88.1c3ba029.js"><link rel="prefetch" href="/assets/js/89.a4797bc8.js"><link rel="prefetch" href="/assets/js/9.1e695694.js"><link rel="prefetch" href="/assets/js/90.12ceffe8.js"><link rel="prefetch" href="/assets/js/91.40c9fc92.js"><link rel="prefetch" href="/assets/js/92.9648154b.js"><link rel="prefetch" href="/assets/js/93.d4e90cec.js"><link rel="prefetch" href="/assets/js/94.50b5eb64.js"><link rel="prefetch" href="/assets/js/95.9d54c125.js"><link rel="prefetch" href="/assets/js/96.fce496cb.js"><link rel="prefetch" href="/assets/js/97.86a56b4e.js"><link rel="prefetch" href="/assets/js/98.cf20fa6d.js"><link rel="prefetch" href="/assets/js/99.92c0c3e3.js">
    <link rel="stylesheet" href="/assets/css/0.styles.a5323663.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">LIANYONGXING. BLOG</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/index.html" class="nav-link">
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="Blog" class="dropdown-title"><span class="title">Blog</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/blog/NLP/" class="nav-link router-link-active">
  NLP
</a></li><li class="dropdown-item"><!----> <a href="/blog/machineLearning/" class="nav-link">
  MachineLearning
</a></li><li class="dropdown-item"><!----> <a href="/blog/Papers/" class="nav-link">
  Papers
</a></li><li class="dropdown-item"><!----> <a href="/blog/recommenderSystem/" class="nav-link">
  RecommenderSystem
</a></li><li class="dropdown-item"><!----> <a href="/blog/ObjectDetection/" class="nav-link">
  ObjectDetection
</a></li><li class="dropdown-item"><!----> <a href="/blog/os/" class="nav-link">
  OperatorSystem
</a></li><li class="dropdown-item"><!----> <a href="/blog/cookies/" class="nav-link">
  Cookies
</a></li></ul></div></div><div class="nav-item"><a href="https://github.com/lianyongxing" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div><div class="nav-item"><a href="/about/" class="nav-link">
  About
</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/index.html" class="nav-link">
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="Blog" class="dropdown-title"><span class="title">Blog</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/blog/NLP/" class="nav-link router-link-active">
  NLP
</a></li><li class="dropdown-item"><!----> <a href="/blog/machineLearning/" class="nav-link">
  MachineLearning
</a></li><li class="dropdown-item"><!----> <a href="/blog/Papers/" class="nav-link">
  Papers
</a></li><li class="dropdown-item"><!----> <a href="/blog/recommenderSystem/" class="nav-link">
  RecommenderSystem
</a></li><li class="dropdown-item"><!----> <a href="/blog/ObjectDetection/" class="nav-link">
  ObjectDetection
</a></li><li class="dropdown-item"><!----> <a href="/blog/os/" class="nav-link">
  OperatorSystem
</a></li><li class="dropdown-item"><!----> <a href="/blog/cookies/" class="nav-link">
  Cookies
</a></li></ul></div></div><div class="nav-item"><a href="https://github.com/lianyongxing" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div><div class="nav-item"><a href="/about/" class="nav-link">
  About
</a></div> <!----></nav>  <ul class="sidebar-links"><li><a href="/blog/NLP/" class="sidebar-link">概述</a></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Transformer</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Adversarial training</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Semi-Supervised-Learning</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>传统文本特征提取方法</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Contrastive Learning</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Embedding的各向同(异)性</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>Attention is ALL you need</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/blog/NLP/Attention is ALL you need.html" class="active sidebar-link">Attention is ALL you need</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/blog/NLP/Attention is ALL you need.html#算法流程" class="sidebar-link">算法流程</a></li><li class="sidebar-sub-header"><a href="/blog/NLP/Attention is ALL you need.html#几点注意" class="sidebar-link">几点注意</a></li><li class="sidebar-sub-header"><a href="/blog/NLP/Attention is ALL you need.html#references" class="sidebar-link">References</a></li></ul></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Adversarial Discriminative Domain Adaptation</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>数据增强——Embedding Mixup</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Long Short Term Memory</span> <span class="arrow right"></span></p> <!----></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="attention-is-all-you-need"><a href="#attention-is-all-you-need" class="header-anchor">#</a> Attention is ALL you need</h1> <h2 id="算法流程"><a href="#算法流程" class="header-anchor">#</a> 算法流程</h2> <p><strong>Attention</strong>是Transformer结构的核心组成部分，在此对self-attention进行介绍和分析</p> <p><strong>Self-Attention</strong>的算法流程很简单清晰，简单概括：定义三个变换QKV，然后用QK计算每个token的score，与V加权即可</p> <ol><li>定义三个线性变换</li></ol> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">SelfAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>module<span class="token punctuation">)</span><span class="token punctuation">:</span>

  <span class="token comment"># 定义三个基本的矩阵线性变换QKV</span>
	self<span class="token punctuation">.</span>query <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span>  <span class="token comment"># 假设输入和输出都是128</span>
	self<span class="token punctuation">.</span>key <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span>  <span class="token comment"># </span>
	self<span class="token punctuation">.</span>value <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span>  <span class="token comment"># </span>
</code></pre></div><ol start="2"><li>对同一个句子，假设单个句子的Embedding为(10, 128)，代表一句话10个单词，每个单词128维向量表征，对这个Embedding分别进行QKV三个变换，维度从(10, 128)*(128, 128) = (10, 128)</li></ol> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">SelfAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>module<span class="token punctuation">)</span><span class="token punctuation">:</span>

  <span class="token comment"># 定义三个基本的矩阵线性变换QKV</span>
	self<span class="token punctuation">.</span>query <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span>  <span class="token comment"># 假设输入和输出都是128</span>
	self<span class="token punctuation">.</span>key <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span>  <span class="token comment"># </span>
	self<span class="token punctuation">.</span>value <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span>  <span class="token comment"># </span>

	<span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> embeddings<span class="token punctuation">)</span><span class="token punctuation">:</span>
		Q <span class="token operator">=</span> self<span class="token punctuation">.</span>query<span class="token punctuation">(</span>embeddings<span class="token punctuation">)</span>   <span class="token comment"># (L, 128)*(128, 128) = (L, 128)</span>
		K <span class="token operator">=</span> self<span class="token punctuation">.</span>key<span class="token punctuation">(</span>embeddings<span class="token punctuation">)</span>
		V <span class="token operator">=</span> self<span class="token punctuation">.</span>value<span class="token punctuation">(</span>embeddings<span class="token punctuation">)</span>
</code></pre></div><ol start="3"><li>根据attention的公式</li></ol> <p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo>)</mo><mo>=</mo><mi>s</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo>(</mo><mfrac><mrow><msup><mi>Q</mi><mrow><mi>T</mi></mrow></msup><mi>K</mi></mrow><mrow><msqrt><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow></msqrt></mrow></mfrac><mo>)</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">Attention(Q, K, V) = softmax(\frac{Q^{T}K}{\sqrt{d_k}})V
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:1.5183309999999999em;"></span><span class="strut bottom" style="height:2.448331em;vertical-align:-0.9300000000000002em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit">A</span><span class="mord mathit">t</span><span class="mord mathit">t</span><span class="mord mathit">e</span><span class="mord mathit">n</span><span class="mord mathit">t</span><span class="mord mathit">i</span><span class="mord mathit">o</span><span class="mord mathit">n</span><span class="mopen">(</span><span class="mord mathit">Q</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mord mathit" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathit">s</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:0.10764em;">f</span><span class="mord mathit">t</span><span class="mord mathit">m</span><span class="mord mathit">a</span><span class="mord mathit">x</span><span class="mopen">(</span><span class="mord reset-textstyle displaystyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class="mfrac"><span class="vlist"><span style="top:0.7472200000000002em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="reset-textstyle textstyle cramped"><span class="mord textstyle cramped"><span class="sqrt mord"><span class="sqrt-sign" style="top:-0.017220000000000013em;"><span class="style-wrap reset-textstyle textstyle uncramped">√</span></span><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="mord textstyle cramped"><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span><span style="top:-0.77722em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="reset-textstyle textstyle uncramped sqrt-line"></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span>​</span></span></span></span></span></span><span style="top:-0.22999999999999998em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style="top:-0.677em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="reset-textstyle textstyle uncramped"><span class="mord textstyle uncramped"><span class="mord"><span class="mord mathit">Q</span><span class="vlist"><span style="top:-0.363em;margin-right:0.05em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">T</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mord mathit" style="margin-right:0.07153em;">K</span></span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mclose">)</span><span class="mord mathit" style="margin-right:0.22222em;">V</span></span></span></span></span></p> <p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">Q</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07153em;">K</span></span></span></span>相乘，在维度上(L, 128) * (L, 128)^T = (L, L) 本质上**相当于计算了Q中每个token对K中每个token的相似度！**也就是说，attention的是利用token之间的相似度来定义的；之后对其进行<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msqrt><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{d_k}</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.8572200000000001em;"></span><span class="strut bottom" style="height:1.04em;vertical-align:-0.18278em;"></span><span class="base textstyle uncramped"><span class="sqrt mord"><span class="sqrt-sign" style="top:-0.017220000000000013em;"><span class="style-wrap reset-textstyle textstyle uncramped">√</span></span><span class="vlist"><span style="top:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="mord textstyle cramped"><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span><span style="top:-0.77722em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span><span class="reset-textstyle textstyle uncramped sqrt-line"></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:1em;">​</span></span>​</span></span></span></span></span></span>的缩放，然后进行softmax归一化，也就是每个<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord mathit">Q</span></span></span></span>中的token对<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.07153em;">K</span></span></span></span>中的全部token的attention score之和为1，同时增加了非线性；</p> <div align="center"><img src="/assets/img/att1.e426b7e9.png" width="120%"></div> <p>最后attention score与<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.68333em;"></span><span class="strut bottom" style="height:0.68333em;vertical-align:0em;"></span><span class="base textstyle uncramped"><span class="mord mathit" style="margin-right:0.22222em;">V</span></span></span></span>相乘，得到最终结果</p> <div align="center"><img src="/assets/img/att2.69d1b6d1.png" width="120%"></div> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">SelfAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>module<span class="token punctuation">)</span><span class="token punctuation">:</span>
		self<span class="token punctuation">.</span>query <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span>  <span class="token comment"># </span>
		self<span class="token punctuation">.</span>key <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span>
		self<span class="token punctuation">.</span>value <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span>
		self<span class="token punctuation">.</span>softmax <span class="token operator">=</span> nn<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

		<span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> embeddings<span class="token punctuation">)</span><span class="token punctuation">:</span>
			q <span class="token operator">=</span> self<span class="token punctuation">.</span>query<span class="token punctuation">(</span>embeddings<span class="token punctuation">)</span>  <span class="token comment"># (10, 128)</span>
			k <span class="token operator">=</span> self<span class="token punctuation">.</span>key<span class="token punctuation">(</span>embeddings<span class="token punctuation">)</span>    <span class="token comment"># (10, 128)</span>
			v <span class="token operator">=</span> self<span class="token punctuation">.</span>value<span class="token punctuation">(</span>embeddings<span class="token punctuation">)</span>  <span class="token comment"># (10, 128)</span>
			attention_scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>Q<span class="token punctuation">,</span> K<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># (10, 10) </span>
            attention_scores <span class="token operator">=</span> attention_scores <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>attention_head_size<span class="token punctuation">)</span>
			attention_probs <span class="token operator">=</span> self<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attention_scores<span class="token punctuation">)</span>
			out <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>attention_probs<span class="token punctuation">,</span> v<span class="token punctuation">)</span>  <span class="token comment"># (10, 128)</span>
			<span class="token keyword">return</span> out
</code></pre></div><h2 id="几点注意"><a href="#几点注意" class="header-anchor">#</a> 几点注意</h2> <ol><li>可以看出，如果<strong>调换一句话中两个字的位置</strong>，实际上在计算attention的时候是没有影响的，也就是attention本质上不考虑位置信息(Textcnn、LSTM这种都有位置信息)，在Bert中是用position embedding来引入位置信息</li> <li>因为QKV都是从一个<strong>句子本身</strong>得到，所以叫self-attention</li> <li><strong>为什么要在softmax之前scaled</strong>?向量的点积会很大，因为如果不进行scaled，值过大，进行softmax后的梯度趋于0，会发生梯度消失(已经过实验验证)</li> <li><strong>为什么用维度<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">d_k</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>来放缩</strong>？因为假设q和k是均值为0，方差为1互相独立的随机变量，那么q*k的均值是0，方差是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">d_k</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>(可由公式推导得到)，方差越大说明点积可能取到很大，因此很自然的方法就是将方差稳定到1，因此除以<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">d_k</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></li></ol> <h2 id="references"><a href="#references" class="header-anchor">#</a> References</h2> <p>[1]. <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener noreferrer">Attention Is All You Need<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></p></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/blog/NLP/Embedding的各向同性.html" class="prev">
        Embedding的各向同(异)性
      </a></span> <span class="next"><a href="/blog/NLP/Adversarial Discriminative Domain Adaptation.html">
        Adversarial Discriminative Domain Adaptation
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.acaacfed.js" defer></script><script src="/assets/js/2.858fe96a.js" defer></script><script src="/assets/js/39.384a454e.js" defer></script>
  </body>
</html>
