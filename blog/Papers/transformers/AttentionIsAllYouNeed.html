<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Transformer:  Attention is all you need | LIANYONGXING. BLOG</title>
    <meta name="description" content="May the force be with u.">
    <meta name="generator" content="VuePress 1.4.0">
    <link rel="icon" href="ficon.jpeg">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/2.10.0/github-markdown.min.css">
    
    <link rel="preload" href="/assets/css/0.styles.a5323663.css" as="style"><link rel="preload" href="/assets/js/app.acaacfed.js" as="script"><link rel="preload" href="/assets/js/2.858fe96a.js" as="script"><link rel="preload" href="/assets/js/24.dc5b3fc5.js" as="script"><link rel="prefetch" href="/assets/js/10.652038c7.js"><link rel="prefetch" href="/assets/js/100.2dd9d0f2.js"><link rel="prefetch" href="/assets/js/101.487a243a.js"><link rel="prefetch" href="/assets/js/102.0c61121f.js"><link rel="prefetch" href="/assets/js/103.4dbe63e3.js"><link rel="prefetch" href="/assets/js/104.7f502b03.js"><link rel="prefetch" href="/assets/js/105.6e83f9e6.js"><link rel="prefetch" href="/assets/js/106.5409dca1.js"><link rel="prefetch" href="/assets/js/107.7ad54acd.js"><link rel="prefetch" href="/assets/js/108.3486f36a.js"><link rel="prefetch" href="/assets/js/109.5ee4cd3b.js"><link rel="prefetch" href="/assets/js/11.4f8ea0df.js"><link rel="prefetch" href="/assets/js/110.6417250e.js"><link rel="prefetch" href="/assets/js/111.ded28c89.js"><link rel="prefetch" href="/assets/js/112.94f5b6c6.js"><link rel="prefetch" href="/assets/js/113.3b1fe42e.js"><link rel="prefetch" href="/assets/js/114.0e643f0c.js"><link rel="prefetch" href="/assets/js/115.32a16f28.js"><link rel="prefetch" href="/assets/js/12.a9ef9aaf.js"><link rel="prefetch" href="/assets/js/13.acff38a0.js"><link rel="prefetch" href="/assets/js/14.113e66b1.js"><link rel="prefetch" href="/assets/js/15.19b59d69.js"><link rel="prefetch" href="/assets/js/16.4ee77b14.js"><link rel="prefetch" href="/assets/js/17.a5b6d566.js"><link rel="prefetch" href="/assets/js/18.744a9950.js"><link rel="prefetch" href="/assets/js/19.fd9e7099.js"><link rel="prefetch" href="/assets/js/20.66903b5c.js"><link rel="prefetch" href="/assets/js/21.c98f5d06.js"><link rel="prefetch" href="/assets/js/22.a0056c91.js"><link rel="prefetch" href="/assets/js/23.73369199.js"><link rel="prefetch" href="/assets/js/25.bf612edb.js"><link rel="prefetch" href="/assets/js/26.b9dd7686.js"><link rel="prefetch" href="/assets/js/27.371379cd.js"><link rel="prefetch" href="/assets/js/28.4a33fe83.js"><link rel="prefetch" href="/assets/js/29.be42af3d.js"><link rel="prefetch" href="/assets/js/3.47952932.js"><link rel="prefetch" href="/assets/js/30.37bd8e9e.js"><link rel="prefetch" href="/assets/js/31.58f89f44.js"><link rel="prefetch" href="/assets/js/32.741f73e3.js"><link rel="prefetch" href="/assets/js/33.a9c7dcb5.js"><link rel="prefetch" href="/assets/js/34.c3f829b5.js"><link rel="prefetch" href="/assets/js/35.29c897d0.js"><link rel="prefetch" href="/assets/js/36.1e02f622.js"><link rel="prefetch" href="/assets/js/37.a1d23a14.js"><link rel="prefetch" href="/assets/js/38.06964860.js"><link rel="prefetch" href="/assets/js/39.384a454e.js"><link rel="prefetch" href="/assets/js/4.7feb14c8.js"><link rel="prefetch" href="/assets/js/40.c7e4dfb7.js"><link rel="prefetch" href="/assets/js/41.3e5c5125.js"><link rel="prefetch" href="/assets/js/42.dcf19498.js"><link rel="prefetch" href="/assets/js/43.54203bcb.js"><link rel="prefetch" href="/assets/js/44.9cccb36c.js"><link rel="prefetch" href="/assets/js/45.3ce2a56c.js"><link rel="prefetch" href="/assets/js/46.17aac222.js"><link rel="prefetch" href="/assets/js/47.57ed0257.js"><link rel="prefetch" href="/assets/js/48.8baebce7.js"><link rel="prefetch" href="/assets/js/49.7eb16be5.js"><link rel="prefetch" href="/assets/js/5.55c31470.js"><link rel="prefetch" href="/assets/js/50.06274b55.js"><link rel="prefetch" href="/assets/js/51.b50e24a8.js"><link rel="prefetch" href="/assets/js/52.7b92239f.js"><link rel="prefetch" href="/assets/js/53.fb7cbb6a.js"><link rel="prefetch" href="/assets/js/54.d047b5f1.js"><link rel="prefetch" href="/assets/js/55.6e2fd120.js"><link rel="prefetch" href="/assets/js/56.fa7ee707.js"><link rel="prefetch" href="/assets/js/57.31b7d0a7.js"><link rel="prefetch" href="/assets/js/58.80cddbc7.js"><link rel="prefetch" href="/assets/js/59.92b2b0f2.js"><link rel="prefetch" href="/assets/js/6.792ce084.js"><link rel="prefetch" href="/assets/js/60.9ab8e458.js"><link rel="prefetch" href="/assets/js/61.b3abbf87.js"><link rel="prefetch" href="/assets/js/62.2bde1ed2.js"><link rel="prefetch" href="/assets/js/63.39cf6423.js"><link rel="prefetch" href="/assets/js/64.c839079e.js"><link rel="prefetch" href="/assets/js/65.ea1754c1.js"><link rel="prefetch" href="/assets/js/66.802ec528.js"><link rel="prefetch" href="/assets/js/67.a30d6fff.js"><link rel="prefetch" href="/assets/js/68.d98ac090.js"><link rel="prefetch" href="/assets/js/69.083b1217.js"><link rel="prefetch" href="/assets/js/7.3964fa52.js"><link rel="prefetch" href="/assets/js/70.272720ba.js"><link rel="prefetch" href="/assets/js/71.386b54d7.js"><link rel="prefetch" href="/assets/js/72.005e615f.js"><link rel="prefetch" href="/assets/js/73.b1266355.js"><link rel="prefetch" href="/assets/js/74.9ab08f58.js"><link rel="prefetch" href="/assets/js/75.eefe4ff0.js"><link rel="prefetch" href="/assets/js/76.e4162716.js"><link rel="prefetch" href="/assets/js/77.e1ead4d7.js"><link rel="prefetch" href="/assets/js/78.ae9b9cc3.js"><link rel="prefetch" href="/assets/js/79.eeaf4627.js"><link rel="prefetch" href="/assets/js/8.5ccbfc45.js"><link rel="prefetch" href="/assets/js/80.0272858c.js"><link rel="prefetch" href="/assets/js/81.bbc0907f.js"><link rel="prefetch" href="/assets/js/82.4de7fcc4.js"><link rel="prefetch" href="/assets/js/83.2ad342e1.js"><link rel="prefetch" href="/assets/js/84.3398b3bb.js"><link rel="prefetch" href="/assets/js/85.be446d7c.js"><link rel="prefetch" href="/assets/js/86.1ef2094d.js"><link rel="prefetch" href="/assets/js/87.53795a29.js"><link rel="prefetch" href="/assets/js/88.1c3ba029.js"><link rel="prefetch" href="/assets/js/89.a4797bc8.js"><link rel="prefetch" href="/assets/js/9.1e695694.js"><link rel="prefetch" href="/assets/js/90.12ceffe8.js"><link rel="prefetch" href="/assets/js/91.40c9fc92.js"><link rel="prefetch" href="/assets/js/92.9648154b.js"><link rel="prefetch" href="/assets/js/93.d4e90cec.js"><link rel="prefetch" href="/assets/js/94.50b5eb64.js"><link rel="prefetch" href="/assets/js/95.9d54c125.js"><link rel="prefetch" href="/assets/js/96.fce496cb.js"><link rel="prefetch" href="/assets/js/97.86a56b4e.js"><link rel="prefetch" href="/assets/js/98.cf20fa6d.js"><link rel="prefetch" href="/assets/js/99.92c0c3e3.js">
    <link rel="stylesheet" href="/assets/css/0.styles.a5323663.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">LIANYONGXING. BLOG</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/index.html" class="nav-link">
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="Blog" class="dropdown-title"><span class="title">Blog</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/blog/NLP/" class="nav-link">
  NLP
</a></li><li class="dropdown-item"><!----> <a href="/blog/machineLearning/" class="nav-link">
  MachineLearning
</a></li><li class="dropdown-item"><!----> <a href="/blog/Papers/" class="nav-link router-link-active">
  Papers
</a></li><li class="dropdown-item"><!----> <a href="/blog/recommenderSystem/" class="nav-link">
  RecommenderSystem
</a></li><li class="dropdown-item"><!----> <a href="/blog/ObjectDetection/" class="nav-link">
  ObjectDetection
</a></li><li class="dropdown-item"><!----> <a href="/blog/os/" class="nav-link">
  OperatorSystem
</a></li><li class="dropdown-item"><!----> <a href="/blog/cookies/" class="nav-link">
  Cookies
</a></li></ul></div></div><div class="nav-item"><a href="https://github.com/lianyongxing" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div><div class="nav-item"><a href="/about/" class="nav-link">
  About
</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/index.html" class="nav-link">
  Home
</a></div><div class="nav-item"><div class="dropdown-wrapper"><button type="button" aria-label="Blog" class="dropdown-title"><span class="title">Blog</span> <span class="arrow right"></span></button> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/blog/NLP/" class="nav-link">
  NLP
</a></li><li class="dropdown-item"><!----> <a href="/blog/machineLearning/" class="nav-link">
  MachineLearning
</a></li><li class="dropdown-item"><!----> <a href="/blog/Papers/" class="nav-link router-link-active">
  Papers
</a></li><li class="dropdown-item"><!----> <a href="/blog/recommenderSystem/" class="nav-link">
  RecommenderSystem
</a></li><li class="dropdown-item"><!----> <a href="/blog/ObjectDetection/" class="nav-link">
  ObjectDetection
</a></li><li class="dropdown-item"><!----> <a href="/blog/os/" class="nav-link">
  OperatorSystem
</a></li><li class="dropdown-item"><!----> <a href="/blog/cookies/" class="nav-link">
  Cookies
</a></li></ul></div></div><div class="nav-item"><a href="https://github.com/lianyongxing" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div><div class="nav-item"><a href="/about/" class="nav-link">
  About
</a></div> <!----></nav>  <ul class="sidebar-links"><li><a href="/blog/Papers/" class="sidebar-link">概述</a></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>Transformer</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/blog/Papers/transformers/AttentionIsAllYouNeed.html" class="active sidebar-link">Transformer:  Attention is all you need</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/blog/Papers/transformers/AttentionIsAllYouNeed.html#embedding" class="sidebar-link">Embedding</a></li><li class="sidebar-sub-header"><a href="/blog/Papers/transformers/AttentionIsAllYouNeed.html#encoder" class="sidebar-link">Encoder</a></li><li class="sidebar-sub-header"><a href="/blog/Papers/transformers/AttentionIsAllYouNeed.html#decoder" class="sidebar-link">Decoder</a></li><li class="sidebar-sub-header"><a href="/blog/Papers/transformers/AttentionIsAllYouNeed.html#transformer的三种attention" class="sidebar-link">Transformer的三种Attention</a></li><li class="sidebar-sub-header"><a href="/blog/Papers/transformers/AttentionIsAllYouNeed.html#关于模型" class="sidebar-link">关于模型</a></li></ul></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Bert</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>GPT</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Roberta</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>ViT</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>MAE</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>MoCo</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Swin Transformer</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Transformer-XL</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Deberta</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>XLNet</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>Clip</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>ViLT</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>InstructGPT</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>MultiModal-01</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>MultiModal-02</span> <span class="arrow right"></span></p> <!----></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="transformer-attention-is-all-you-need"><a href="#transformer-attention-is-all-you-need" class="header-anchor">#</a> Transformer:  Attention is all you need</h1> <ol><li><strong>Vaswani, Ashish, et al. &quot;Attention is all you need.&quot; <em>Advances in neural information processing systems</em> 30 (2017).APA</strong> <ol><li>提出了个简单的结构，仅仅依赖<strong>注意力机制</strong>，而没有用循环（RNN）和卷积（CONV）</li> <li>RNN系列的缺点：
<ol><li><strong>结构无法并行</strong>，下一个词需要依赖上一个词的结果，计算性能差；</li> <li>历史信息需要一步步往后传，如果时序比较长，<strong>早期的信息可能在后面丢失</strong>，如果不想丢失，那么每一步的结果都要保存，占用很多资源</li></ol></li> <li>其他工作，比如用卷积神经网络替换RNN，**卷积神经网络对比较长的序列难以建模，**但是本文的Transformer可以一次看到所有信息；<strong>但是卷积网络可以有多个输出通道，每个通道可以识别不同模式，因此Transformer中加了MultiHead-Attention的机制</strong></li> <li>在本文工作之前，Attention是跟 RNN一起用的，主要用在如何把编码器的东西传给解码器；<strong>本文工作是只用Attention</strong>，因此就可以并行计算，而且效果很好</li></ol></li></ol> <div align="center"><img src="/assets/img/Untitled.41cf3694.png" width="70%"></div> <h2 id="embedding"><a href="#embedding" class="header-anchor">#</a> Embedding</h2> <h3 id="embedding-2"><a href="#embedding-2" class="header-anchor">#</a> Embedding</h3> <p>编码和解码的embedding是一样的，把权重乘了根号<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">d_k</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>，因为在学embedding的时候，可能将embedding的L2long学的比较小，但后面会加上position embedding，因此把两个放到近似相同的量级上（都是-1到1之间）</p> <h3 id="position-embedding"><a href="#position-embedding" class="header-anchor">#</a> Position Embedding</h3> <p>Attention是没有序列的位置信息的，因此在embedding这里加进去；使用不同周期的cos函数来表征的，因此在-1和1之前抖动</p> <p>(对比<em>RNN是上个时刻的输出作为下个时刻的输入，因此本身就是时序的)</em></p> <h2 id="encoder"><a href="#encoder" class="header-anchor">#</a> Encoder</h2> <h3 id="基本架构"><a href="#基本架构" class="header-anchor">#</a> 基本架构</h3> <p>Encoder部分包含6个Layer，每个Layer由2个sub-Layer，每个sub-Layer包含一个MultiHead-Attention和一个point-wised feed forward network，每个sub-Layer后都有先接一个残差，然后LayerNorm，因此每个sub-Layer最终为：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo>(</mo><mi>x</mi><mo>+</mo><mi>S</mi><mi>u</mi><mi>b</mi><mi>l</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">LayerNorm(x+Sublayer(x))</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base textstyle uncramped"><span class="mord mathit">L</span><span class="mord mathit">a</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mord mathit">e</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit" style="margin-right:0.10903em;">N</span><span class="mord mathit">o</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mord mathit">m</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mbin">+</span><span class="mord mathit" style="margin-right:0.05764em;">S</span><span class="mord mathit">u</span><span class="mord mathit">b</span><span class="mord mathit" style="margin-right:0.01968em;">l</span><span class="mord mathit">a</span><span class="mord mathit" style="margin-right:0.03588em;">y</span><span class="mord mathit">e</span><span class="mord mathit" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span></p> <ul><li>Encoder = 6 * Layers
<ul><li>Layer = Multihead-Attention + FFN</li></ul></li></ul> <div align="center"><img src="/assets/img/Untitled 1.a4314e61.png" width="80%"></div> <p>图2. scaled Dot-Product Attention 和 Multi-head-Attention</p> <h3 id="scaled-dot-product-attention"><a href="#scaled-dot-product-attention" class="header-anchor">#</a> <strong>Scaled Dot-Product Attention</strong></h3> <p>Attention是计算一个query和key的相似度来得到attention score；之前的工作有不同Attention，比如add-attention和dot-product-attention，作者这里的改进是做了一个放缩，将Q*K的结果用根号<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">d_k</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>进行缩放，因此这里作者取名<strong>Scaled Dot-Product Attention</strong>，缩放的原因是因为当<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">d_k</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.69444em;"></span><span class="strut bottom" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit">d</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style="margin-right:0.03148em;">k</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>过大时，即向量长度更长，点积后的结果可能很大和很小，因此Softmax最大的值就更靠近1，其他值更靠近0，此时梯度比较小，会很难训练，因此缩放是个比较好的方式</p> <div align="center"><img src="/assets/img/Untitled 2.e2e87ae9.png" width="70%"></div>
    图3. Attention矩阵相乘的过程
<h3 id="masked-multi-head-attention"><a href="#masked-multi-head-attention" class="header-anchor">#</a> Masked Multi-head Attention</h3> <p>Masked Multi-head Attention中的<strong>Mask</strong>是因为在计算t时刻的attention，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>q</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">q_t</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.43056em;"></span><span class="strut bottom" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="base textstyle uncramped"><span class="mord"><span class="mord mathit" style="margin-right:0.03588em;">q</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.03588em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">t</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span>时，此时只能看到前k-1时刻的情况，理论上是不知道后面时刻的值的，因此将后面的值Mask掉，具体就是取一个很负的值，此时softmax出来的值对应的权重都为0，只有k-1时刻和之前时刻的值；Multi-head是对于单个head的attention，不如<strong>将其投影到多个比较低的维度上去</strong>，对于每个维度的输出，在最终并到一起，再投影会来得到最终的输出；</p> <p><strong>为什么用Multi-head Attention？</strong> 是因为dot-product本身是没什么参数可以学的，就是两个点积，但是这里的linear的权重w是可以学习的，网络内部通过不同方式去处理；也就是给你h次机会，使得在投影进去的度量空间里匹配不同模式</p> <h3 id="self-attention"><a href="#self-attention" class="header-anchor">#</a> Self-Attention</h3> <p>Self-Attention：这里就是q、k、v都是自己，QK就相当计算每个token的相似度作为score，最终加权到v上去，这里肯定是自己跟自己的score最大，但是其他词对当前词也会有影响，而且分成多个head就投影到多个空间上去，肯定可以学到不同空间的影响</p> <h3 id="point-wise-feed-forward-network"><a href="#point-wise-feed-forward-network" class="header-anchor">#</a> Point-wise Feed Forward Network</h3> <p>作者将一句话中的每个词看作一个个点（point），因此名字叫point-wise；实际上就是个MLP，单个隐藏层，把输入从512扩大到2048，然后再缩放回去，这里的max就是激活函数Relu</p> <p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>F</mi><mi>F</mi><mi>N</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo>(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><msub><mi>W</mi><mn>1</mn></msub><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><mo>)</mo><msub><mi>W</mi><mn>2</mn></msub><mo>+</mo><msub><mi>b</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">FFN(x) = max(0, xW_1+b_1)W_2+b_2
</annotation></semantics></math></span><span aria-hidden="true" class="katex-html"><span class="strut" style="height:0.75em;"></span><span class="strut bottom" style="height:1em;vertical-align:-0.25em;"></span><span class="base displaystyle textstyle uncramped"><span class="mord mathit" style="margin-right:0.13889em;">F</span><span class="mord mathit" style="margin-right:0.13889em;">F</span><span class="mord mathit" style="margin-right:0.10903em;">N</span><span class="mopen">(</span><span class="mord mathit">x</span><span class="mclose">)</span><span class="mrel">=</span><span class="mord mathit">m</span><span class="mord mathit">a</span><span class="mord mathit">x</span><span class="mopen">(</span><span class="mord mathrm">0</span><span class="mpunct">,</span><span class="mord mathit">x</span><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">W</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mbin">+</span><span class="mord"><span class="mord mathit">b</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">1</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathit" style="margin-right:0.13889em;">W</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:-0.13889em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span><span class="mbin">+</span><span class="mord"><span class="mord mathit">b</span><span class="vlist"><span style="top:0.15em;margin-right:0.05em;margin-left:0em;"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathrm">2</span></span></span><span class="baseline-fix"><span class="fontsize-ensurer reset-size5 size5"><span style="font-size:0em;">​</span></span>​</span></span></span></span></span></span></span></p> <p>这里的作用就是前面Attention已经对于每个token都把这个序列信息进行了一个抽取，然后这里通过对每个token的序列信息进行进一步加工，然后再输出</p> <p><em><strong>这里跟RNN的区别就是Transformer是通过attention将一句话完整的序列信息整合到每个token上，而rnn是每次将上一时刻的信息传给下个时刻，然后跟当前时刻的词一起输入；因此序列信息的整合方式不同，两者都用了MLP做最终的信息整合</strong></em></p> <h2 id="decoder"><a href="#decoder" class="header-anchor">#</a> Decoder</h2> <p>Decoder部分也是包括6个Layer，除了Encoder中相同的2个sub-Layer，在这之前，还插入了第三个sub-Layer：Masked-Multihead-Attention模块；同样每个sub-Layer后都有先接一个残差，然后LayerNorm；</p> <ul><li>Decoder = 6 * Layers
<ul><li>Layer =Masked-Multihead-Attention +  Multihead-Attention + FFN</li></ul></li></ul> <h2 id="transformer的三种attention"><a href="#transformer的三种attention" class="header-anchor">#</a> Transformer的三种Attention</h2> <div align="center"><img src="/assets/img/Untitled 3.0042ac92.png" width="80%"></div> <p>图4. Transformer中的3种Attention</p> <p>整个Transformer有三种Attention</p> <ul><li>encoder中的self-attention</li> <li>decoder中的masked self-attention</li> <li>decoder中解码时的attention
<ul><li>query为mask-attention的输出，key、value为encoder的输出，相当于看query和训练到的key的相似度，得到attention score，最终作用在学习到的value上，产生最终的输出</li></ul></li></ul> <h2 id="关于模型"><a href="#关于模型" class="header-anchor">#</a> 关于模型</h2> <ul><li>题目名字为“Attention is All you need”，实际上不只是Attention，少了MLP、残差、position都训不出东西来</li> <li>原本图像用cnn，文本用rnn，这个模型统一都用attention提取特征了，某种程度上的统一</li> <li>attention的基本假设太简单了，因此对数据信息的抓取能力变差了，因此需要多层，多数据，导致现在的模型都必须很大，数据需要很多</li></ul></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/blog/Papers/" class="prev router-link-active">
        概述
      </a></span> <span class="next"><a href="/blog/Papers/bert/BERT Pre-training of Deep Bidirectional Transformer.html">
        BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.acaacfed.js" defer></script><script src="/assets/js/2.858fe96a.js" defer></script><script src="/assets/js/24.dc5b3fc5.js" defer></script>
  </body>
</html>
