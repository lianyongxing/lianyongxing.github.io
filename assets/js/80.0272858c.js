(window.webpackJsonp=window.webpackJsonp||[]).push([[80],{654:function(e,r,t){"use strict";t.r(r);var n=t(44),l=Object(n.a)({},(function(){var e=this,r=e.$createElement,t=e._self._c||r;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("h1",{attrs:{id:"albert-a-lite-bert-for-self-supervised-learning-of-language-representations"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#albert-a-lite-bert-for-self-supervised-learning-of-language-representations"}},[e._v("#")]),e._v(" Albert: A lite bert for self-supervised learning of language representations")]),e._v(" "),t("ol",[t("li",[e._v("Lan Z, Chen M, Goodman S, et al. Albert: A lite bert for self-supervised learning of language representations[J]. arXiv preprint arXiv:1909.11942, 2019.\n"),t("ol",[t("li",[e._v("随着模型尺寸的增加，预训练的效果在下游任务上往往更好，但是从某些方面来说，这也意味着跟大的显存的压力和更长的训练时间，因此作者提出了两种参数缩减技术\n"),t("ol",[t("li",[e._v("embedding层的参数矩阵分解")]),e._v(" "),t("li",[e._v("跨层参数共享，参数共享作为一种形式的正则化可以稳定训练和提高泛化能力")])])]),e._v(" "),t("li",[e._v("作者为了提高albert的性能，提出了一种学习句间连贯性的预训练任务Sentence Order Prediction（SOP），移除了NSP")]),e._v(" "),t("li",[e._v("去掉了dropout，因为前面的参数共享以及去掉了很多参数了，训练的时候发现不加dropout也不过拟合")])])])])])}),[],!1,null,null,null);r.default=l.exports}}]);