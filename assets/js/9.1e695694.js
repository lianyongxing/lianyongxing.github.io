(window.webpackJsonp=window.webpackJsonp||[]).push([[9],{450:function(t,e,a){t.exports=a.p+"assets/img/Untitled.4a883819.png"},451:function(t,e,a){t.exports=a.p+"assets/img/Untitled 1.e65968a5.png"},452:function(t,e,a){t.exports=a.p+"assets/img/Untitled 2.08e9dc4f.png"},453:function(t,e,a){t.exports=a.p+"assets/img/Untitled 3.b3bd827d.png"},454:function(t,e,a){t.exports=a.p+"assets/img/Untitled 4.590a5270.png"},455:function(t,e,a){t.exports=a.p+"assets/img/Untitled 5.b41256a5.png"},456:function(t,e,a){t.exports=a.p+"assets/img/Untitled 6.1ec336e8.png"},457:function(t,e,a){t.exports=a.p+"assets/img/Untitled 7.79971774.png"},668:function(t,e,a){"use strict";a.r(e);var i=a(44),n=Object(i.a)({},(function(){var t=this,e=t.$createElement,i=t._self._c||e;return i("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[i("h1",{attrs:{id:"multimodal-2"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#multimodal-2"}},[t._v("#")]),t._v(" MultiModal(2)")]),t._v(" "),i("h2",{attrs:{id:"blip"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#blip"}},[t._v("#")]),t._v(" BLIP")]),t._v(" "),i("p",[i("strong",[t._v("unified vision-language understanding and generation")])]),t._v(" "),i("div",{attrs:{align:"center"}},[i("img",{attrs:{src:a(450),width:"90%"}})]),t._v(" "),i("p",[t._v("图1. caption和filter module")]),t._v(" "),i("ol",[i("li",[i("p",[t._v("Li J, Li D, Xiong C, et al. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation[C]//International Conference on Machine Learning. PMLR, 2022: 12888-12900.")]),t._v(" "),i("ol",[i("li",[t._v("**贡献一：**从网上爬下来的数据往往是很多噪声，作者先用这些数据训练一个模型，然后再通过一些方法得到更干净的数据，用这些更干净的数据得到更好的模型")]),t._v(" "),i("li",[i("strong",[t._v("贡献二："),i("strong",[t._v("Unified Model，将理解和生成任务统一。当时有两个研究动机："),i("strong",[t._v("模型层面")]),t._v("和")]),t._v("数据层面")]),t._v("；\n"),i("ol",[i("li",[t._v("模型层面，当时主流的架构有两种，encoder-only和encoder-decoder架构，第一种往往理解任务好，但是生成任务不好，第二种虽然能做生成，但是没有统一框架，又很难做image-text-retrival这种任务，因此作者考虑设计一个统一的框架实现不同任务")]),t._v(" "),i("li",[t._v("数据层面，效果比较好的方法都是在大规模网上爬取的noisy数据集上训练的，虽然数据集变大，可以抵消部分噪声的影响，但是作者觉得直接用这些还是不好的，不是最优解，因此作者提出了"),i("strong",[t._v("Captioner")]),t._v("和"),i("strong",[t._v("Filter")]),t._v("两个module，captioner就是给一个图片，自动生成一段字幕，从而得到大量合成数据，之后filter module将不匹配的image-text对从数据集中删掉")])])]),t._v(" "),i("li",[t._v("图片是一个标准的"),i("a",{attrs:{href:"https://www.notion.so/ViT-An-image-is-worth-16x16-words-Transformers-for-image-recognition-at-scale-9b4bd3ae0d2942499f57c54792c6af63",target:"_blank",rel:"noopener noreferrer"}},[t._v("Vit模型"),i("OutboundLink")],1),t._v("，文本的三个任务分别用了不同的transformer encoder结构做loss，作者称为MED**（Mixture of encoder and decoder）**")]),t._v(" "),i("li",[t._v("作者将已经训练好的BLIP模型拿出来，然后将图像模型和两个文本模型在coco数据集上做微调，微调过后的MED即为filter，作者用filter计算image-text的相似度，ITM的分数，就知道文本和图像是否match，非match的就直接扔掉；captioner就是原来的LM模型，用它来对图片生成描述，有时比原来的图文对效果还好；"),i("em",[t._v("作者发现优化后的数据集提高很明显")])])]),t._v(" "),i("div",{attrs:{align:"center"}},[i("img",{attrs:{src:a(451),width:"100%"}})]),t._v(" "),i("p",[t._v("图2. BLIP架构")]),t._v(" "),i("div",{attrs:{align:"center"}},[i("img",{attrs:{src:a(452),width:"100%"}})]),t._v(" "),i("p",[t._v("图3. captioner和filter")])])]),t._v(" "),i("div",{attrs:{align:"center"}},[i("img",{attrs:{src:a(453),width:"100%"}})]),t._v(" "),i("p",[t._v("图4. capfilter效果")]),t._v(" "),i("h2",{attrs:{id:"coca"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#coca"}},[t._v("#")]),t._v(" CoCa")]),t._v(" "),i("p",[i("strong",[t._v("Contrastive Captioners")])]),t._v(" "),i("div",{attrs:{align:"center"}},[i("img",{attrs:{src:a(454),width:"100%"}})]),t._v(" "),i("p",[t._v("图5. coca架构")]),t._v(" "),i("div",{attrs:{align:"center"}},[i("img",{attrs:{src:a(455),width:"90%"}})]),t._v(" "),i("p",[t._v("图6. coca结果图")]),t._v(" "),i("ol",[i("li",[t._v("Yu J, Wang Z, Vasudevan V, et al. Coca: Contrastive captioners are image-text foundation models[J]. arXiv preprint arXiv:2205.01917, 2022.\n"),i("ol",[i("li",[t._v("从题目就可以看出由两个loss得到，"),i("strong",[t._v("contrastive loss")]),t._v("和"),i("strong",[t._v("caption loss")]),t._v("，跟blip很像，数据更大，效果更好")]),t._v(" "),i("li",[t._v("作者在文本端直接输入mask文本，而不是分别输入mask和非mask的文本，因此避免了每次计算loss的多次forward，因为作者用的数据太多了，所以可以抵消mask的影响，因为模型基本上什么都见过了")])])])]),t._v(" "),i("h2",{attrs:{id:"beitv3"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#beitv3"}},[t._v("#")]),t._v(" BeiTv3")]),t._v(" "),i("p",[t._v("相当于之前一系列工作的合集：BEIT、BEITv2、VLBEIT、VLMO……")]),t._v(" "),i("p",[i("strong",[t._v("将图像也看作一种语言 Image as a foreign language")])]),t._v(" "),i("div",{attrs:{align:"center"}},[i("img",{attrs:{src:a(456),width:"90%"}})]),t._v("\n![图7. 实际上跟VLMO架构一样MoME](MultiModal(2)%204e32ae7d27484599840f04ad8ea396f0/Untitled%206.png)\n"),i("p",[t._v("图7. 实际上跟VLMO架构一样MoME")]),t._v(" "),i("div",{attrs:{align:"center"}},[i("img",{attrs:{src:a(457),width:"100%"}})]),t._v(" "),i("p",[t._v("图8. 提取不同的encoder部分做不同的下游任务")]),t._v(" "),i("ol",[i("li",[t._v("Wang W, Bao H, Dong L, et al. Image as a foreign language: Beit pretraining for all vision and vision-language tasks[J]. arXiv preprint arXiv:2208.10442, 2022.MLA\n"),i("ol",[i("li",[t._v("目标是想做更大一统的框架"),i("strong",[t._v("Big convergence")]),t._v("，从模型和目标函数上都统一，不同scale的数据都统一；因此，"),i("strong",[t._v("数据层面")]),t._v("：作者将图像也看作一种语言**（Imglish），"),i("strong",[t._v("因此也不需要ITC、ITM这些loss了，只用一个MLM LOSS就ok了；"),i("strong",[t._v("模型方面")]),t._v("，用了之前的MoME架构，重新起了个名字叫")]),t._v("Multiway Model；**成为了非常简单有效可扩展的框架")])])])]),t._v(" "),i("p",[t._v("最开始图像的encoder是复杂的目标检测器，之后ViT尝试将transformer用到图片中，多模态将文本和图片整合，都用了transformer，提出了ViLT；同期出现了CLIP模型，进行图文retrieval，之后ALBEF发现CLIP比较适合做retrieval，ViLT结构比较简单；因此ALBEF结合了三者的长处，提出了综合fusion-encoder的模式，取得了不错的效果；在之后coca只用contrast loss和caption loss做多模态；另外一个分支是ViLT之后的VLMO，通过共享参数的方式做统一多模态框架，提出了BLIP；Vit当时也尝试用mask data language，但是效果不太好，因此其他研究者提出了BEIT，用MLM，之后将BERT和BEIT结合在一起，提出了VL-BEIT；最后在总结了前面所有工作的基础上，提出了大一统模型BEITv3")]),t._v(" "),i("p",[t._v("再后来就是真正大一统的模型，PaLi，通过prompt调整输出")]),t._v(" "),i("p",[t._v("chatgpt、gpt-4")])])}),[],!1,null,null,null);e.default=n.exports}}]);