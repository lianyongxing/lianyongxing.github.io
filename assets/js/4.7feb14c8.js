(window.webpackJsonp=window.webpackJsonp||[]).push([[4],{403:function(t,s,a){t.exports=a.p+"assets/img/FAB60E2A-0C7B-498D-A46F-E14C92C811CA.cd72743c.jpg"},404:function(t,s,a){t.exports=a.p+"assets/img/E6731EFB-9547-44E5-B859-2DDAEEFAFBDA.4247ec87.png"},405:function(t,s,a){t.exports=a.p+"assets/img/6337EABA-A6AA-4B8F-B007-D52C3F39DC33.2329cd01.png"},406:function(t,s,a){t.exports=a.p+"assets/img/D3801086-0F96-454C-A6AE-C49AE96FAF25.197bd431.png"},407:function(t,s,a){t.exports=a.p+"assets/img/AEA09B74-A0E3-45C8-B952-30B218BD5729.91514c91.png"},408:function(t,s,a){t.exports=a.p+"assets/img/43AD0168-8096-4E9F-87B0-BB1CFCF1BE42.3adeff9c.png"},409:function(t,s,a){t.exports=a.p+"assets/img/562460D0-8FCF-47A7-9BA7-A2205AAC12C5.c4345f2a.png"},410:function(t,s,a){t.exports=a.p+"assets/img/D415D4B8-A105-4B15-BEF5-4C57069EDE79.ca4a2f0d.png"},411:function(t,s,a){t.exports=a.p+"assets/img/AAC8ED85-A88E-436F-8D94-B05F01590AC5.ae626181.png"},412:function(t,s,a){t.exports=a.p+"assets/img/image-20200707160115010.d6ee5bf6.png"},413:function(t,s,a){t.exports=a.p+"assets/img/image-20200707161443093.dab6d58b.png"},414:function(t,s,a){t.exports=a.p+"assets/img/image-20200707162734216.e74fe279.png"},415:function(t,s,a){t.exports=a.p+"assets/img/77A11318-0284-435C-8780-35AF0B36E9F0.132020ab.png"},655:function(t,s,a){"use strict";a.r(s);var i=a(44),e=Object(i.a)({},(function(){var t=this,s=t.$createElement,i=t._self._c||s;return i("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[i("h1",{attrs:{id:"深度网络演进"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#深度网络演进"}},[t._v("#")]),t._v(" 深度网络演进")]),t._v(" "),i("div",{attrs:{align:"center"}},[i("img",{attrs:{src:a(403),width:"80%"}})]),t._v(" "),i("p",[t._v("最早的深度网络，仅包含各个普通的层的CNN，卷积层，激活函数，池化层，全连接层")]),t._v(" "),i("h2",{attrs:{id:"alexnet"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#alexnet"}},[t._v("#")]),t._v(" AlexNet")]),t._v(" "),i("div",{attrs:{align:"center"}},[i("img",{attrs:{src:a(404),width:"80%"}})]),t._v(" "),i("p",[t._v("Alexnet是2012的ISLVRC冠军，准确率由70%提高到80%左右，使用GPU训练")]),t._v(" "),i("h3",{attrs:{id:"主要亮点"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#主要亮点"}},[t._v("#")]),t._v(" 主要亮点")]),t._v(" "),i("ul",[i("li",[t._v("使用GPU训练")]),t._v(" "),i("li",[t._v("使用了Relu函数")]),t._v(" "),i("li",[t._v("使用了LRN局部响应归一化")]),t._v(" "),i("li",[t._v("在全连阶层使用dropout随机失活神经元，以减少过拟合")])]),t._v(" "),i("h2",{attrs:{id:"vgg"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#vgg"}},[t._v("#")]),t._v(" VGG")]),t._v(" "),i("div",{attrs:{align:"center"}},[i("img",{attrs:{src:a(405),width:"66%"}})]),t._v(" "),i("p",[t._v("2014年推出，ImageNet获奖")]),t._v(" "),i("p",[t._v("网络中的亮点，通过堆叠多个3x3的卷积核来代替大尺度卷积核（减少所需的参数），从而使网络层次更深")]),t._v(" "),i("p",[t._v("可以使用堆叠2个3x3的卷积核来替代5x5的卷积核，堆叠3个3x3的卷积核来替代7x7的卷积核。（具有相同的感受野）")]),t._v(" "),i("p",[t._v("感受野计算公式：")]),t._v(" "),i("p",[i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",[i("semantics",[i("mrow",[i("mi",[t._v("R")]),i("msub",[i("mi",[t._v("F")]),i("mrow",[i("mi",[t._v("i")]),i("mo",[t._v("+")]),i("mn",[t._v("1")])],1)],1),i("mo",[t._v("=")]),i("mi",[t._v("R")]),i("msub",[i("mi",[t._v("F")]),i("mrow",[i("mi",[t._v("i")])],1)],1),i("mo",[t._v("+")]),i("mo",[t._v("(")]),i("mi",[t._v("k")]),i("mi",[t._v("s")]),i("mi",[t._v("i")]),i("mi",[t._v("z")]),i("mi",[t._v("e")]),i("mo",[t._v("−")]),i("mn",[t._v("1")]),i("mo",[t._v(")")]),i("mo",[t._v("∗")]),i("mi",[t._v("s")]),i("mi",[t._v("t")]),i("mi",[t._v("r")]),i("mi",[t._v("i")]),i("mi",[t._v("d")]),i("mi",[t._v("e")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("RF_{i+1} = RF_{i}+(ksize-1)*stride")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"strut",staticStyle:{height:"0.75em"}}),i("span",{staticClass:"strut bottom",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),i("span",{staticClass:"base textstyle uncramped"},[i("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.00773em"}},[t._v("R")]),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.13889em"}},[t._v("F")]),i("span",{staticClass:"vlist"},[i("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"-0.13889em"}},[i("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[i("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),i("span",{staticClass:"reset-textstyle scriptstyle cramped"},[i("span",{staticClass:"mord scriptstyle cramped"},[i("span",{staticClass:"mord mathit"},[t._v("i")]),i("span",{staticClass:"mbin"},[t._v("+")]),i("span",{staticClass:"mord mathrm"},[t._v("1")])])])]),i("span",{staticClass:"baseline-fix"},[i("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[i("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])]),i("span",{staticClass:"mrel"},[t._v("=")]),i("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.00773em"}},[t._v("R")]),i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.13889em"}},[t._v("F")]),i("span",{staticClass:"vlist"},[i("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"-0.13889em"}},[i("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[i("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),i("span",{staticClass:"reset-textstyle scriptstyle cramped"},[i("span",{staticClass:"mord scriptstyle cramped"},[i("span",{staticClass:"mord mathit"},[t._v("i")])])])]),i("span",{staticClass:"baseline-fix"},[i("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[i("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])]),i("span",{staticClass:"mbin"},[t._v("+")]),i("span",{staticClass:"mopen"},[t._v("(")]),i("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.03148em"}},[t._v("k")]),i("span",{staticClass:"mord mathit"},[t._v("s")]),i("span",{staticClass:"mord mathit"},[t._v("i")]),i("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.04398em"}},[t._v("z")]),i("span",{staticClass:"mord mathit"},[t._v("e")]),i("span",{staticClass:"mbin"},[t._v("−")]),i("span",{staticClass:"mord mathrm"},[t._v("1")]),i("span",{staticClass:"mclose"},[t._v(")")]),i("span",{staticClass:"mbin"},[t._v("∗")]),i("span",{staticClass:"mord mathit"},[t._v("s")]),i("span",{staticClass:"mord mathit"},[t._v("t")]),i("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.02778em"}},[t._v("r")]),i("span",{staticClass:"mord mathit"},[t._v("i")]),i("span",{staticClass:"mord mathit"},[t._v("d")]),i("span",{staticClass:"mord mathit"},[t._v("e")])])])])]),t._v(" "),i("p",[t._v("第一层 1")]),t._v(" "),i("p",[t._v("第二层 （F(1)-1）*stride + ksize")]),t._v(" "),i("p",[t._v("第三层 （F(2)-1）*stride + ksize")]),t._v(" "),i("p",[t._v("第n层 （F(n-1)-1）*stride + ksize")]),t._v(" "),i("p",[t._v("感受野的概念")]),t._v(" "),i("div",{attrs:{align:"center"}},[i("img",{attrs:{src:a(406),width:"75%"}})]),t._v(" "),i("h2",{attrs:{id:"googlenet"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#googlenet"}},[t._v("#")]),t._v(" GoogLeNet")]),t._v(" "),i("ul",[i("li",[t._v("引入了Inception结构")]),t._v(" "),i("li",[t._v("使用1x1卷积核进行降维以及映射处理")]),t._v(" "),i("li",[t._v("添加两个辅助分类器进行训练")]),t._v(" "),i("li",[t._v("丢弃全连接层，使用平均池化层（大大减少模型参数）")])]),t._v(" "),i("h3",{attrs:{id:"inception结构"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#inception结构"}},[t._v("#")]),t._v(" Inception结构")]),t._v(" "),i("p",[t._v("初始的Inception结构")]),t._v(" "),i("div",{attrs:{align:"center"}},[i("img",{attrs:{src:a(407),width:"60%"}})]),t._v(" "),i("p",[t._v("不同分支得到不同尺度的特征矩阵，每个分支得到的特征矩阵的高和宽必须相同（在深度上进行拼接）")]),t._v(" "),i("p",[t._v("添加了降维模块的Inception结构"),i("br")]),i("div",{attrs:{align:"center"}},[i("br"),t._v(" "),i("img",{attrs:{src:a(408),width:"60%"}}),i("br")]),i("p"),t._v(" "),i("p",[t._v("使用1x1的卷积核进行降维，降维可以抽取特征点和主要特征")]),t._v(" "),i("h3",{attrs:{id:"辅助分类器"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#辅助分类器"}},[t._v("#")]),t._v(" 辅助分类器")]),t._v(" "),i("p",[t._v("在中间输出层接一个分类器，作为辅助分类，因此在训练的时候有3个loss")]),t._v(" "),i("h2",{attrs:{id:"resnet"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#resnet"}},[t._v("#")]),t._v(" ResNet")]),t._v(" "),i("p",[t._v("Resnet2015年提出，ImageNet第一名")]),t._v(" "),i("p",[t._v("网络亮点")]),t._v(" "),i("ul",[i("li",[t._v("超深的网络结构（突破1000层）")]),t._v(" "),i("li",[t._v("提出残差模块")]),t._v(" "),i("li",[t._v("使用batchnormal加速训练（丢弃dropout）")])]),t._v(" "),i("p",[t._v("过深的网络")]),t._v(" "),i("ul",[i("li",[t._v("存在梯度消失和梯度爆炸（梯度剪切/正则、激活函数、BN、预训练微调，残差网络，LSTM）")]),t._v(" "),i("li",[t._v("退化问题")])]),t._v(" "),i("h3",{attrs:{id:"残差模块"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#残差模块"}},[t._v("#")]),t._v(" 残差模块")]),t._v(" "),i("div",{attrs:{align:"center"}},[i("img",{attrs:{src:a(409),width:"60%"}})]),t._v(" "),i("p",[t._v("主分支与shortcut输出特征矩阵的shape必须相同，因此如果是虚线的话，就是先做一个变维操作，再进行结合。")]),t._v(" "),i("p",[t._v("残差一半比较小，学习残差难度小一点")]),t._v(" "),i("div",{attrs:{align:"center"}},[i("img",{attrs:{src:a(410),width:"60%"}})]),t._v(" "),i("p",[t._v("这个操作可以使得低维的信息为高维的信息做一个补充，并且防止梯度消失")]),t._v(" "),i("h3",{attrs:{id:"batchnorm"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#batchnorm"}},[t._v("#")]),t._v(" BatchNorm")]),t._v(" "),i("p",[t._v("在图像输入时，一般会对图像进行标准化处理，这样能加速网络的收敛，对于第一个conv层，输入是满足某个分布的特征矩阵，但其输出就变了，作为下一个conv的输入时，不满足某个分布规律了（这里指的是整个训练样本集对应的feature map的数据要满足的分布规律）。而batchnorm就是使feature map满足均值为0，方差为1的分布律。")]),t._v(" "),i("div",{attrs:{align:"center"}},[i("img",{attrs:{src:a(411),width:"80%"}})]),t._v(" "),i("p",[t._v("对于一批数据，计算均值和方差，然后将数据进行归一化，归一化后再用两个参数进行线性调节，因为均值为0，方差为1的数据的效果不一定是最好的，这两个参数是通过反向传播进行学习得到的。")]),t._v(" "),i("h4",{attrs:{id:"bn需要注意的问题"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#bn需要注意的问题"}},[t._v("#")]),t._v(" BN需要注意的问题")]),t._v(" "),i("ul",[i("li",[t._v("训练时将traing设置为True，验证时设置为False")]),t._v(" "),i("li",[t._v("batchsize设置的尽可能大，在小的时候表现不好，设置大的求的的均值和方差接近整个训练集的均值和方差")]),t._v(" "),i("li",[t._v("论文中建议将BN层放在Conv和激活函数之间，且卷积层不要使用bias，因为没有用（调整后的值和调整前的值一样）")])]),t._v(" "),i("p",[t._v("因为输出如果有bias，则归一化的时候减去均值时，也会有bias，两者抵消，而方差相同，因此输出相同。")]),t._v(" "),i("h2",{attrs:{id:"mobilenet"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#mobilenet"}},[t._v("#")]),t._v(" MobileNet")]),t._v(" "),i("p",[t._v("mobilenet是2017年提出的，专注于移动端或者嵌入式设备中的轻量CNN网络，准确率小幅度降低的情况下大大减少了模型参数的运算量")]),t._v(" "),i("p",[t._v("网络亮点")]),t._v(" "),i("ul",[i("li",[t._v("Depthwise Convolution（大大减少运算量和参数数量）DW卷积")]),t._v(" "),i("li",[t._v("增加超参数 控制卷积核的个数的参数alpha，控制输入图像大小的beta")])]),t._v(" "),i("h4",{attrs:{id:"dw卷积（depthwise-conv）"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#dw卷积（depthwise-conv）"}},[t._v("#")]),t._v(" DW卷积（Depthwise Conv）")]),t._v(" "),i("p",[t._v("卷积核channel = 1（每个卷积核负责1个channel ）")]),t._v(" "),i("p",[t._v("输入特征矩阵channel = 卷积核个数 = 输出特征矩阵channel")]),t._v(" "),i("h4",{attrs:{id:"深度可分的卷积（depthwise-separable-conv）"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#深度可分的卷积（depthwise-separable-conv）"}},[t._v("#")]),t._v(" 深度可分的卷积（Depthwise Separable Conv）")]),t._v(" "),i("p",[t._v("由DW卷积（DepthWise）和PW卷积（PointsWise）组成")]),t._v(" "),i("p",[t._v("PW卷积就是普通卷积，只不过尺寸是1x1")]),t._v(" "),i("div",{attrs:{align:"center"}},[i("img",{attrs:{src:a(412),width:"80%"}})]),t._v(" "),i("h3",{attrs:{id:"mobilenet-v1"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#mobilenet-v1"}},[t._v("#")]),t._v(" MobileNet v1")]),t._v(" "),i("p",[t._v("v1版本集成了DW卷积和PW卷积，引入两个参数"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",[i("semantics",[i("mrow",[i("mi",[t._v("α")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\alpha")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"strut",staticStyle:{height:"0.43056em"}}),i("span",{staticClass:"strut bottom",staticStyle:{height:"0.43056em","vertical-align":"0em"}}),i("span",{staticClass:"base textstyle uncramped"},[i("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.0037em"}},[t._v("α")])])])]),t._v("控制卷积核的个数，"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",[i("semantics",[i("mrow",[i("mi",[t._v("β")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\beta")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"strut",staticStyle:{height:"0.69444em"}}),i("span",{staticClass:"strut bottom",staticStyle:{height:"0.8888799999999999em","vertical-align":"-0.19444em"}}),i("span",{staticClass:"base textstyle uncramped"},[i("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.05278em"}},[t._v("β")])])])]),t._v("控制输入图片的size，虽然在精度上有一点下降，但是大大降低了模型参数和计算量，但在实际训练的时候，发现depthwise部分卷积核会被废掉，即卷积核参数大部分为0，这个在v2版本进行了改善")]),t._v(" "),i("h3",{attrs:{id:"mobilenet-v2"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#mobilenet-v2"}},[t._v("#")]),t._v(" MobileNet v2")]),t._v(" "),i("p",[t._v("相比v1准确率更高，模型更小。")]),t._v(" "),i("p",[t._v("亮点主要有两个：")]),t._v(" "),i("ul",[i("li",[t._v("Inverted Residual block（倒残差结构）")]),t._v(" "),i("li",[t._v("Linear Bottlenecks")])]),t._v(" "),i("p",[t._v("回顾：残差结构将输入通过1x1的卷积核，降低输入特征的channel，然后正常卷积，最后再通过一个1x1的卷积核扩充channnl，即步骤包括1x1降维、卷积、1x1升维。")]),t._v(" "),i("p",[t._v("A. 倒残差结构")]),t._v(" "),i("p",[t._v("先通过一个1x1的卷积核实现channel升维，再通过3x3的DW卷积，最后通过1x1的卷积核channel降维，正好跟残差结构是相反的，因此叫倒残差结构。需要注意的是，在普通残差网络中，激活函数为RELU，而在倒残差结构中，使用的激活函数为RELU6")]),t._v(" "),i("p",[i("span",{staticClass:"katex-display"},[i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",[i("semantics",[i("mrow",[i("mi",[t._v("R")]),i("mi",[t._v("e")]),i("mi",[t._v("l")]),i("mi",[t._v("u")]),i("mn",[t._v("6")]),i("mo",[t._v("=")]),i("mi",[t._v("m")]),i("mi",[t._v("i")]),i("mi",[t._v("n")]),i("mo",[t._v("(")]),i("mi",[t._v("m")]),i("mi",[t._v("a")]),i("mi",[t._v("x")]),i("mo",[t._v("(")]),i("mi",[t._v("x")]),i("mo",{attrs:{separator:"true"}},[t._v(",")]),i("mn",[t._v("0")]),i("mo",[t._v(")")]),i("mo",{attrs:{separator:"true"}},[t._v(",")]),i("mn",[t._v("6")]),i("mo",[t._v(")")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("Relu6 = min(max(x, 0), 6)\n")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"strut",staticStyle:{height:"0.75em"}}),i("span",{staticClass:"strut bottom",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),i("span",{staticClass:"base displaystyle textstyle uncramped"},[i("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.00773em"}},[t._v("R")]),i("span",{staticClass:"mord mathit"},[t._v("e")]),i("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.01968em"}},[t._v("l")]),i("span",{staticClass:"mord mathit"},[t._v("u")]),i("span",{staticClass:"mord mathrm"},[t._v("6")]),i("span",{staticClass:"mrel"},[t._v("=")]),i("span",{staticClass:"mord mathit"},[t._v("m")]),i("span",{staticClass:"mord mathit"},[t._v("i")]),i("span",{staticClass:"mord mathit"},[t._v("n")]),i("span",{staticClass:"mopen"},[t._v("(")]),i("span",{staticClass:"mord mathit"},[t._v("m")]),i("span",{staticClass:"mord mathit"},[t._v("a")]),i("span",{staticClass:"mord mathit"},[t._v("x")]),i("span",{staticClass:"mopen"},[t._v("(")]),i("span",{staticClass:"mord mathit"},[t._v("x")]),i("span",{staticClass:"mpunct"},[t._v(",")]),i("span",{staticClass:"mord mathrm"},[t._v("0")]),i("span",{staticClass:"mclose"},[t._v(")")]),i("span",{staticClass:"mpunct"},[t._v(",")]),i("span",{staticClass:"mord mathrm"},[t._v("6")]),i("span",{staticClass:"mclose"},[t._v(")")])])])])])]),t._v(" "),i("div",{attrs:{align:"center"}},[i("img",{attrs:{src:a(413),width:"80%"}})]),t._v(" "),i("p",[t._v("B. linear Bottlenecks")]),t._v(" "),i("p",[t._v("对于倒残差结构中最后一个1x1的卷积层，采用了线性激活函数，而非RELU，原论文中，RELU对低维特征信息造成较大损失。")]),t._v(" "),i("p",[t._v("作者在论文中做了一个实验，用一个单通道的图片作为输入，然后用多层矩阵"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",[i("semantics",[i("mrow",[i("mi",[t._v("T")])],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("T")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"strut",staticStyle:{height:"0.68333em"}}),i("span",{staticClass:"strut bottom",staticStyle:{height:"0.68333em","vertical-align":"0em"}}),i("span",{staticClass:"base textstyle uncramped"},[i("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.13889em"}},[t._v("T")])])])]),t._v("作卷积进行特征提取，提取高维特征后，用RELU输出，然后再用"),i("span",{staticClass:"katex"},[i("span",{staticClass:"katex-mathml"},[i("math",[i("semantics",[i("mrow",[i("msup",[i("mi",[t._v("T")]),i("mrow",[i("mo",[t._v("−")]),i("mn",[t._v("1")])],1)],1)],1),i("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("T^{-1}")])],1)],1)],1),i("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[i("span",{staticClass:"strut",staticStyle:{height:"0.8141079999999999em"}}),i("span",{staticClass:"strut bottom",staticStyle:{height:"0.8141079999999999em","vertical-align":"0em"}}),i("span",{staticClass:"base textstyle uncramped"},[i("span",{staticClass:"mord"},[i("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.13889em"}},[t._v("T")]),i("span",{staticClass:"vlist"},[i("span",{staticStyle:{top:"-0.363em","margin-right":"0.05em"}},[i("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[i("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),i("span",{staticClass:"reset-textstyle scriptstyle uncramped"},[i("span",{staticClass:"mord scriptstyle uncramped"},[i("span",{staticClass:"mord"},[t._v("−")]),i("span",{staticClass:"mord mathrm"},[t._v("1")])])])]),i("span",{staticClass:"baseline-fix"},[i("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[i("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])])])])]),t._v("进行反变换，企图得到最初的输入。当提取2、3层时，发现做反变换后，得到的输出图像相对于输入图像有很大的缺失，而提取到10层以上的时候，才可以相对完整的还原出原始图像，因此觉得RELU对低维的信息会产生损失。而由于倒残差结构两边细，中间粗的结构，输出的是一个低维的信息，因此不使用RELU激活函数，而使用线性激活函数。"),i("br")]),i("div",{attrs:{align:"center"}},[i("br"),t._v(" "),i("img",{attrs:{src:a(414),width:"80%"}}),i("br")]),i("p"),t._v(" "),i("p",[t._v("注意：在倒残差结构中，只有输入stride为1，而且输入特征矩阵和输出特征矩阵shape相等的时候才有shortcut连接")]),t._v(" "),i("h2",{attrs:{id:"迁移学习"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#迁移学习"}},[t._v("#")]),t._v(" 迁移学习")]),t._v(" "),i("div",{attrs:{align:"center"}},[i("img",{attrs:{src:a(415),width:"80%"}})])])}),[],!1,null,null,null);s.default=e.exports}}]);