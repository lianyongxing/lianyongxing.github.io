(window.webpackJsonp=window.webpackJsonp||[]).push([[15],{432:function(t,s,a){t.exports=a.p+"assets/img/Untitled.65a11103.png"},433:function(t,s,a){t.exports=a.p+"assets/img/Untitled 1.16edad84.png"},434:function(t,s,a){t.exports=a.p+"assets/img/Untitled 2.1a2876b7.png"},435:function(t,s,a){t.exports=a.p+"assets/img/Untitled 3.3572a8bf.png"},436:function(t,s,a){t.exports=a.p+"assets/img/Untitled 4.068cdaa5.png"},437:function(t,s,a){t.exports=a.p+"assets/img/Untitled 5.f9a0534a.png"},663:function(t,s,a){"use strict";a.r(s);var e=a(44),i=Object(e.a)({},(function(){var t=this,s=t.$createElement,e=t._self._c||s;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h1",{attrs:{id:"gpt：generative-pre-training"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#gpt：generative-pre-training"}},[t._v("#")]),t._v(" GPT：generative pre-training")]),t._v(" "),e("h1",{attrs:{id:"gpt-1"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#gpt-1"}},[t._v("#")]),t._v(" "),e("strong",[t._v("GPT-1")])]),t._v(" "),e("h2",{attrs:{id:"background"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#background"}},[t._v("#")]),t._v(" Background")]),t._v(" "),e("p",[e("strong",[t._v("Improving language understanding by generative pre-training")])]),t._v(" "),e("ol",[e("li",[t._v("Radford A, Narasimhan K, Salimans T, et al. Improving language understanding by generative pre-training[J]. 2018.\n"),e("ol",[e("li",[t._v("无标注的数据有很多，标注数据很少，因此本文是通过无标注数据预训练，然后在有标注数据上微调")]),t._v(" "),e("li",[t._v("之前的方法都是学词向量，但是遇到不同任务时，还要做不同的下游模型，这里作者统一用transformer的decoder结构，只需要改变输入格式就能应对不同任务，无需再改模型结构")]),t._v(" "),e("li",[t._v("当时的困难有两个，一是不清楚用什么目标函数，虽然有LM、QA等各种任务，但是往往一种方式在另一个任务上效果就不太好了，没有一个统一好的目标函数；二是如何选择一种有效的将学习到的表征迁移到子任务上；")])])])]),t._v(" "),e("p",[t._v("作者最终选择了Transformer作为基础架构，因为发现transformer在不同任务的迁移学习上更robust，原因是其结构化的记忆模块能够处理更长的文本信息")]),t._v(" "),e("h2",{attrs:{id:"unsupervised-pre-training"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#unsupervised-pre-training"}},[t._v("#")]),t._v(" Unsupervised pre-training")]),t._v(" "),e("p",[t._v("GPT使用标准的语言模型在无标注的数据上进行预训练，极大化似然函数（已知前k-1时刻的值，预测k时刻的值），可以如下表示，U为窗口内的所有词，乘一个W参数，加上一个位置权重W，然后一起通过transformer block，得到一个输出"),e("span",{staticClass:"katex"},[e("span",{staticClass:"katex-mathml"},[e("math",[e("semantics",[e("mrow",[e("msub",[e("mi",[t._v("h")]),e("mi",[t._v("l")])],1)],1),e("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("h_l")])],1)],1)],1),e("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[e("span",{staticClass:"strut",staticStyle:{height:"0.69444em"}}),e("span",{staticClass:"strut bottom",staticStyle:{height:"0.84444em","vertical-align":"-0.15em"}}),e("span",{staticClass:"base textstyle uncramped"},[e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit"},[t._v("h")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"0em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.01968em"}},[t._v("l")])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])])])])]),t._v("，把这个输出乘上一个参数矩阵W，最后softmax，结果就是当前时刻值的概率分布")]),t._v(" "),e("p",[e("span",{staticClass:"katex-display"},[e("span",{staticClass:"katex"},[e("span",{staticClass:"katex-mathml"},[e("math",[e("semantics",[e("mrow",[e("msub",[e("mi",[t._v("h")]),e("mn",[t._v("0")])],1),e("mo",[t._v("=")]),e("mi",[t._v("U")]),e("msub",[e("mi",[t._v("W")]),e("mi",[t._v("e")])],1),e("mo",[t._v("+")]),e("msub",[e("mi",[t._v("W")]),e("mi",[t._v("P")])],1)],1),e("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("h_0 = UW_e + W_P\n")])],1)],1)],1),e("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[e("span",{staticClass:"strut",staticStyle:{height:"0.69444em"}}),e("span",{staticClass:"strut bottom",staticStyle:{height:"0.84444em","vertical-align":"-0.15em"}}),e("span",{staticClass:"base displaystyle textstyle uncramped"},[e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit"},[t._v("h")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"0em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord mathrm"},[t._v("0")])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])]),e("span",{staticClass:"mrel"},[t._v("=")]),e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.10903em"}},[t._v("U")]),e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.13889em"}},[t._v("W")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"-0.13889em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord mathit"},[t._v("e")])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])]),e("span",{staticClass:"mbin"},[t._v("+")]),e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.13889em"}},[t._v("W")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"-0.13889em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.13889em"}},[t._v("P")])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])])])])])])]),t._v(" "),e("p",[e("span",{staticClass:"katex-display"},[e("span",{staticClass:"katex"},[e("span",{staticClass:"katex-mathml"},[e("math",[e("semantics",[e("mrow",[e("msub",[e("mi",[t._v("h")]),e("mi",[t._v("l")])],1),e("mo",[t._v("=")]),e("mi",[t._v("t")]),e("mi",[t._v("r")]),e("mi",[t._v("a")]),e("mi",[t._v("n")]),e("mi",[t._v("s")]),e("mi",[t._v("f")]),e("mi",[t._v("o")]),e("mi",[t._v("r")]),e("mi",[t._v("m")]),e("mi",[t._v("e")]),e("msub",[e("mi",[t._v("r")]),e("mrow",[e("mi",[t._v("b")]),e("mi",[t._v("l")]),e("mi",[t._v("o")]),e("mi",[t._v("c")]),e("mi",[t._v("k")])],1)],1),e("mo",[t._v("(")]),e("msub",[e("mi",[t._v("h")]),e("mrow",[e("mi",[t._v("l")]),e("mo",[t._v("−")]),e("mn",[t._v("1")])],1)],1),e("mo",[t._v(")")])],1),e("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("h_l = transformer_{block}(h_{l-1})\n")])],1)],1)],1),e("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[e("span",{staticClass:"strut",staticStyle:{height:"0.75em"}}),e("span",{staticClass:"strut bottom",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),e("span",{staticClass:"base displaystyle textstyle uncramped"},[e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit"},[t._v("h")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"0em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.01968em"}},[t._v("l")])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])]),e("span",{staticClass:"mrel"},[t._v("=")]),e("span",{staticClass:"mord mathit"},[t._v("t")]),e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.02778em"}},[t._v("r")]),e("span",{staticClass:"mord mathit"},[t._v("a")]),e("span",{staticClass:"mord mathit"},[t._v("n")]),e("span",{staticClass:"mord mathit"},[t._v("s")]),e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.10764em"}},[t._v("f")]),e("span",{staticClass:"mord mathit"},[t._v("o")]),e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.02778em"}},[t._v("r")]),e("span",{staticClass:"mord mathit"},[t._v("m")]),e("span",{staticClass:"mord mathit"},[t._v("e")]),e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.02778em"}},[t._v("r")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"-0.02778em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord scriptstyle cramped"},[e("span",{staticClass:"mord mathit"},[t._v("b")]),e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.01968em"}},[t._v("l")]),e("span",{staticClass:"mord mathit"},[t._v("o")]),e("span",{staticClass:"mord mathit"},[t._v("c")]),e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.03148em"}},[t._v("k")])])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])]),e("span",{staticClass:"mopen"},[t._v("(")]),e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit"},[t._v("h")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"0em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord scriptstyle cramped"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.01968em"}},[t._v("l")]),e("span",{staticClass:"mbin"},[t._v("−")]),e("span",{staticClass:"mord mathrm"},[t._v("1")])])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])]),e("span",{staticClass:"mclose"},[t._v(")")])])])])])]),t._v(" "),e("p",[e("span",{staticClass:"katex-display"},[e("span",{staticClass:"katex"},[e("span",{staticClass:"katex-mathml"},[e("math",[e("semantics",[e("mrow",[e("mi",[t._v("P")]),e("mo",[t._v("(")]),e("mi",[t._v("u")]),e("mo",[t._v(")")]),e("mo",[t._v("=")]),e("mi",[t._v("s")]),e("mi",[t._v("o")]),e("mi",[t._v("f")]),e("mi",[t._v("t")]),e("mi",[t._v("m")]),e("mi",[t._v("a")]),e("mi",[t._v("x")]),e("mo",[t._v("(")]),e("msub",[e("mi",[t._v("h")]),e("mi",[t._v("n")])],1),e("msubsup",[e("mi",[t._v("W")]),e("mi",[t._v("e")]),e("mi",[t._v("T")])],1),e("mo",[t._v(")")])],1),e("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("P(u) = softmax(h_nW_e^T)\n")])],1)],1)],1),e("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[e("span",{staticClass:"strut",staticStyle:{height:"0.8913309999999999em"}}),e("span",{staticClass:"strut bottom",staticStyle:{height:"1.1413309999999999em","vertical-align":"-0.25em"}}),e("span",{staticClass:"base displaystyle textstyle uncramped"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.13889em"}},[t._v("P")]),e("span",{staticClass:"mopen"},[t._v("(")]),e("span",{staticClass:"mord mathit"},[t._v("u")]),e("span",{staticClass:"mclose"},[t._v(")")]),e("span",{staticClass:"mrel"},[t._v("=")]),e("span",{staticClass:"mord mathit"},[t._v("s")]),e("span",{staticClass:"mord mathit"},[t._v("o")]),e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.10764em"}},[t._v("f")]),e("span",{staticClass:"mord mathit"},[t._v("t")]),e("span",{staticClass:"mord mathit"},[t._v("m")]),e("span",{staticClass:"mord mathit"},[t._v("a")]),e("span",{staticClass:"mord mathit"},[t._v("x")]),e("span",{staticClass:"mopen"},[t._v("(")]),e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit"},[t._v("h")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"0em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord mathit"},[t._v("n")])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])]),e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.13889em"}},[t._v("W")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.247em","margin-left":"-0.13889em","margin-right":"0.05em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord mathit"},[t._v("e")])])]),e("span",{staticStyle:{top:"-0.4129999999999999em","margin-right":"0.05em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle uncramped"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.13889em"}},[t._v("T")])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])]),e("span",{staticClass:"mclose"},[t._v(")")])])])])])]),t._v(" "),e("p",[e("strong",[t._v("这里跟Bert的区别是，bert是有掩码的语言模型（MLM），在预测其中某个词的时候可以看到整个句子；而GPT是标准语言模型，预测当前词的时候，只能看见之前的词，后面的词模型看不到；")])]),t._v(" "),e("p",[t._v("（"),e("strong",[t._v("因此GPT是完全的预测未来，难度比bert高很多，bert相当于完形填空）")])]),t._v(" "),e("h2",{attrs:{id:"supervised-fine-tune"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#supervised-fine-tune"}},[t._v("#")]),t._v(" Supervised fine-tune")]),t._v(" "),e("p",[t._v("微调的时候是有标号的，每次给一个序列，预测y；实际上作者在微调的时候发现，将目标函数设置为一起预测标号和预测下一个词，效果更好")]),t._v(" "),e("p",[t._v("微调的目标函数确定好之后，下一个问题就是如何将不同任务统一为这种形式："),e("strong",[t._v("有标号的数据和对应的标号y")])]),t._v(" "),e("h2",{attrs:{id:"task-specific-input-transformations"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#task-specific-input-transformations"}},[t._v("#")]),t._v(" Task-specific input transformations")]),t._v(" "),e("div",{attrs:{align:"center"}},[e("img",{attrs:{src:a(432),width:"100%"}})]),t._v(" "),e("p",[t._v("图1. 将不同任务统一为句子+label的形式")]),t._v(" "),e("p",[t._v("图1展示了如何将不同任务统一形式输入模型做微调，"),e("strong",[t._v("核心实际上就是把不同文本串成一个序列")])]),t._v(" "),e("h1",{attrs:{id:"gpt-2"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#gpt-2"}},[t._v("#")]),t._v(" "),e("strong",[t._v("GPT-2")])]),t._v(" "),e("p",[e("strong",[t._v("Language models are unsupervised multitask learners 语言模型是无监督的多任务学习器")])]),t._v(" "),e("ol",[e("li",[t._v('Liu, Pengfei, et al. "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing." '),e("em",[t._v("ACM Computing Surveys")]),e("br"),t._v("\n 55.9 (2023): 1-35.\n"),e("ol",[e("li",[t._v("gpt被bert打败了，因此想把模型做的更大，数据更多，构建了百万数据集WebText，模型变为15亿参数；但是仅仅加大模型和数据是不够的；")]),t._v(" "),e("li",[t._v("当时业界主流是在一个数据集上训练好，然后去对应的任务上测试，因为模型的泛化性不太好；第二是当时有多任务学习（Multi-task learning）的方法，就是在多个数据集上一起训练，然后构造多个损失函数，可以提高点泛化性，但是这种方式在nlp上效果也一般，因为同样需要很多不同标注数据，而且也要微调")]),t._v(" "),e("li",[t._v("因此作者提出了一个"),e("strong",[t._v("zero-shot")]),t._v("的设定，就是只拼预训练，不需要下游任务的信息，直接做预测看效果，比模型的泛化能力")])])])]),t._v(" "),e("h2",{attrs:{id:"跟gpt-1的不同"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#跟gpt-1的不同"}},[t._v("#")]),t._v(" 跟GPT-1的不同")]),t._v(" "),e("p",[t._v("因为不看下游fine-tune，没有标号数据了，因此要预测的东西和模式，必须在预训练的时候都加入，因此作者举了个例子，比如要进行翻译任务的时候，将输入设定为 (translate to french, english text, french text)，这样的话，再次进行翻译的时候，先输入模型一个translate to french，模型就能知道要翻译后面的东西了；再比如阅读理解的时候，输入为(answer the question, document,"),e("br"),t._v("\nquestion, answer)，模型看到answer the question的时候就知道要回答问题了")]),t._v(" "),e("h2",{attrs:{id:"datasets"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#datasets"}},[t._v("#")]),t._v(" Datasets")]),t._v(" "),e("p",[t._v("爬取了大量数据，但是数据很脏，因此取了别人点赞数高的数据作为高质量数据")]),t._v(" "),e("h2",{attrs:{id:"结果"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#结果"}},[t._v("#")]),t._v(" 结果")]),t._v(" "),e("div",{attrs:{align:"center"}},[e("img",{attrs:{src:a(433),width:"120%"}})]),t._v(" "),e("p",[t._v("图2. GPT-2在不同任务上的表现")]),t._v(" "),e("p",[t._v("从结果上看，模型越大，效果越好，所以还是存在可能，"),e("strong",[t._v("把模型做的更大，理解能力更强的")])]),t._v(" "),e("h1",{attrs:{id:"gpt-3"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#gpt-3"}},[t._v("#")]),t._v(" GPT-3")]),t._v(" "),e("p",[e("strong",[t._v("Language models are few-shot learners 语言模型是few-shot学习器")])]),t._v(" "),e("ol",[e("li",[t._v('Brown, Tom, et al. "Language models are few-shot learners." '),e("em",[t._v("Advances in neural information processing systems")]),t._v(" 33 (2020): 1877-1901.APA\n"),e("ol",[e("li",[t._v("GPT-2没有用任何下游数据finetune，GPT-3还是类似GPT-2的理念，不比微调，而是通过预训练来看模型的效果；但是实际上像人也是看到一些例子后，理解能力会更强，因此作者考虑用少量的数据来训练，也就是few-shot learning")]),t._v(" "),e("li",[e("strong",[t._v("大力出奇迹，搞了1750个亿参数的模型，模型大小和数据都是GPT-2的一百倍")])]),t._v(" "),e("li",[t._v("提出个概念meta-learning和in-context learning")])])])]),t._v(" "),e("div",{attrs:{align:"center"}},[e("img",{attrs:{src:a(434),width:"80%"}})]),t._v(" "),e("p",[t._v("图3. 在不同任务上的效果")]),t._v(" "),e("p",[e("strong",[t._v("在zero-shot、one-shot和few-shot上的不同效果，并且随着参数增加，效果一直提高；")])]),t._v(" "),e("div",{attrs:{align:"center"}},[e("img",{attrs:{src:a(435),width:"100%"}})]),t._v(" "),e("p",[t._v("图4. 不同任务是怎么做的")]),t._v(" "),e("p",[t._v("在不同任务的时候，"),e("strong",[t._v("给了不同的任务提示")]),t._v("，用“⇒”告诉模型可以输出了；每次不用给到的数据不参与微调模型的参数")]),t._v(" "),e("h2",{attrs:{id:"架构"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#架构"}},[t._v("#")]),t._v(" 架构")]),t._v(" "),e("div",{attrs:{align:"center"}},[e("img",{attrs:{src:a(436),width:"100%"}})]),t._v(" "),e("p",[t._v("图5. 不同size的GPT")]),t._v(" "),e("p",[t._v("架构跟GPT-2差不多，主要是加了层，其他的是用了sparse transformer的相似结构，用了pre-normalization")]),t._v(" "),e("div",{attrs:{align:"center"}},[e("img",{attrs:{src:a(437),width:"100%"}})]),t._v(" "),e("p",[t._v("图6. GPT3的数据集")]),t._v(" "),e("h2",{attrs:{id:"limitation"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#limitation"}},[t._v("#")]),t._v(" Limitation")]),t._v(" "),e("ul",[e("li",[t._v("模型是真的学到了东西还是从大量数据中去检索到的")]),t._v(" "),e("li",[t._v("对生成效果不太好")]),t._v(" "),e("li",[t._v("训练太难了。。")])]),t._v(" "),e("h1",{attrs:{id:"gpt-4"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#gpt-4"}},[t._v("#")]),t._v(" GPT-4")]),t._v(" "),e("p",[e("a",{attrs:{href:"https://openai.com/research/gpt-4",target:"_blank",rel:"noopener noreferrer"}},[t._v("GPT-4"),e("OutboundLink")],1)])])}),[],!1,null,null,null);s.default=i.exports}}]);