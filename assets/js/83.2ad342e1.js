(window.webpackJsonp=window.webpackJsonp||[]).push([[83],{671:function(e,t,n){"use strict";n.r(t);var r=n(44),a=Object(r.a)({},(function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[n("h1",{attrs:{id:"xlnet-generalized-autoregressive-pretraining-for-language-understanding"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#xlnet-generalized-autoregressive-pretraining-for-language-understanding"}},[e._v("#")]),e._v(" XLNET: Generalized autoregressive pretraining for language understanding")]),e._v(" "),n("ol",[n("li",[n("p",[e._v("Yang Z, Dai Z, Yang Y, et al. Xlnet: Generalized autoregressive pretraining for language understanding[J]. Advances in neural information processing systems, 2019, 32.")]),e._v(" "),n("ol",[n("li",[e._v("主流的模型有AR和AE两种：AR代表为GPT，自回归模型，但是无法对文本进行双向建模；AE代表为BERT，自编码模型，bert虽然是双向语言模型，但是bert中使用mask是现实文本中不存在的，因此会使finetune衰退，而且由于mask机制，破坏了句子完整性，使得不能对句子的联合概率进行建模，也就是认为mask掉的单词和其他单词是独立的，这过度简化了语言的高阶和长距离依赖的特性")]),e._v(" "),n("li",[e._v("xlnet根据AR和AE的不足，提出最大化序列的最大似然，即考虑所有因子的排列顺序；通过文本的排列，可以学习每个token的左右信息，每个位置能够从所有位置学习语义信息，捕捉双向文本")]),e._v(" "),n("li",[e._v("其次，xlnet不用mask文本，因此不会像bert一样损坏训练数据，而且自回归的方式可以对文本进行联合概率上的建模，而不需要bert那么高度的假设")]),e._v(" "),n("li",[e._v("xlnet整合了transformer-xl的片段递归机制和相对位置编码方案，提高了对长文本的效果")]),e._v(" "),n("li",[e._v("直接将transformer-xl结合排列语言建模是不work的，而且目标模糊，因此作者重新参数化了transformer-xl")]),e._v(" "),n("li",[e._v("最终相当于BERT+GPT+Transformer-XL\n"),n("ol",[n("li",[e._v("使用PLM，具有双向语言建模能力")]),e._v(" "),n("li",[e._v("自回归机制，根据句子生成的方式对语言建模，符合现实")]),e._v(" "),n("li",[e._v("transformer-xl解决长文本问题")])])])]),e._v(" "),n("h2",{attrs:{id:"permutation-language-model（plm）"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#permutation-language-model（plm）"}},[e._v("#")]),e._v(" Permutation Language Model（PLM）")]),e._v(" "),n("p",[e._v("还是按照AR自回归的方式，预测下一个单词x，只不过，在attention的时候，随机在原始单词x的左右随机选取一些token，放到x前面去，其他没选到的词被Mask掉；这样不止可以学习到文本上下文的信息，也不会破坏训练数据；")]),e._v(" "),n("p",[e._v("比如有N个token，一共有N！中排列方式，此时采样M种排列顺序去训练，那么某些词就可以学到跟其他词相互之间的依赖关系")]),e._v(" "),n("p",[e._v("具体实现的时候，作者采用了"),n("strong",[e._v("双流attention机制")])]),e._v(" "),n("ul",[n("li",[e._v("content stream内容流self- attention跟原始bert中相同")]),e._v(" "),n("li",[e._v("query stream流不包含内容信息，相当于把[MASK]占位符扔掉了，只保留这个token的位置信息作query")])]),e._v(" "),n("p",[e._v("这里打乱顺序是为在预测某个单词的时候，可以看到上下文信息；同时又是从左向右自回归得预测")]),e._v(" "),n("p",[e._v("XLNET相对位置编码的优秀解释："),n("a",{attrs:{href:"http://fancyerii.github.io/2019/07/20/xlnet-codes3/#transformer_xl%E6%9E%84%E9%80%A0%E5%87%BD%E6%95%B0",target:"_blank",rel:"noopener noreferrer"}},[e._v("http://fancyerii.github.io/2019/07/20/xlnet-codes3/#transformer_xl构造函数"),n("OutboundLink")],1)])])])])}),[],!1,null,null,null);t.default=a.exports}}]);