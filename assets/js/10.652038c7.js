(window.webpackJsonp=window.webpackJsonp||[]).push([[10],{465:function(t,s,a){t.exports=a.p+"assets/img/Untitled.891e780e.png"},466:function(t,s,a){t.exports=a.p+"assets/img/Untitled 1.3c8275a7.png"},467:function(t,s,a){t.exports=a.p+"assets/img/Untitled 2.fa0645b1.png"},468:function(t,s,a){t.exports=a.p+"assets/img/Untitled 3.554947b7.png"},469:function(t,s,a){t.exports=a.p+"assets/img/Untitled 4.5d65f367.png"},470:function(t,s,a){t.exports=a.p+"assets/img/Untitled 5.f1b5979d.png"},471:function(t,s,a){t.exports=a.p+"assets/img/Untitled 6.22662ead.png"},472:function(t,s,a){t.exports=a.p+"assets/img/Untitled 7.b3228c9d.png"},672:function(t,s,a){"use strict";a.r(s);var n=a(44),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,n=t._self._c||s;return n("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[n("h1",{attrs:{id:"transformer-xl-attentive-language-models-beyond-a-fixed-length-context"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#transformer-xl-attentive-language-models-beyond-a-fixed-length-context"}},[t._v("#")]),t._v(" Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context")]),t._v(" "),n("div",{attrs:{align:"center"}},[n("img",{attrs:{src:a(465),width:"100%"}})]),t._v(" "),n("p",[t._v("图1. 传统的固定长度")]),t._v(" "),n("ol",[n("li",[t._v("Dai Z, Yang Z, Yang Y, et al. Transformer-xl: Attentive language models beyond a fixed-length context[J]. arXiv preprint arXiv:1901.02860, 2019.\n"),n("ol",[n("li",[t._v("transformer的一个出发点是解决长文本的依赖问题，但是一个很大的制约因素是文本长度；之前LSTM大概可以处理200个词左右，因此这块的提升空间很大")]),t._v(" "),n("li",[t._v("作者提出了两种方法实现长文本的依赖\n"),n("ol",[n("li",[t._v("使用片段递归机制，将一个长文本切分，之前每个片段的隐状态都保存，当计算某个时刻的片段时，同时引入上一个时刻的隐信息")]),t._v(" "),n("li",[t._v("使用相对位置编码")])])])])])]),t._v(" "),n("div",{attrs:{align:"center"}},[n("img",{attrs:{src:a(466),width:"100%"}})]),t._v(" "),n("p",[t._v("图2. 使用片段递归机制的transformer")]),t._v(" "),n("h2",{attrs:{id:"具体做法"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#具体做法"}},[t._v("#")]),t._v(" 具体做法")]),t._v(" "),n("p",[t._v("借鉴苏神的博客："),n("a",{attrs:{href:"https://kexue.fm/archives/8130",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://kexue.fm/archives/8130"),n("OutboundLink")],1)]),t._v(" "),n("p",[t._v("XLNET相对位置编码的优秀解释："),n("a",{attrs:{href:"http://fancyerii.github.io/2019/07/20/xlnet-codes3/#transformer_xl%E6%9E%84%E9%80%A0%E5%87%BD%E6%95%B0",target:"_blank",rel:"noopener noreferrer"}},[t._v("http://fancyerii.github.io/2019/07/20/xlnet-codes3/#transformer_xl构造函数"),n("OutboundLink")],1)]),t._v(" "),n("h3",{attrs:{id:"传统相对位置编码"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#传统相对位置编码"}},[t._v("#")]),t._v(" 传统相对位置编码")]),t._v(" "),n("p",[t._v("对于transformer中常规attention：")]),t._v(" "),n("div",{attrs:{align:"center"}},[n("img",{attrs:{src:a(467),width:"35%"}})]),t._v(" "),n("p",[t._v("计算attention score时，对内部进行展开")]),t._v(" "),n("div",{attrs:{align:"center"}},[n("img",{attrs:{src:a(468),width:"80%"}})]),t._v(" "),n("div",{attrs:{align:"center"}},[n("img",{attrs:{src:a(469),width:"90%"}})]),t._v(" "),n("div",{attrs:{align:"center"}},[n("img",{attrs:{src:a(470),width:"90%"}})]),t._v(" "),n("div",{attrs:{align:"center"}},[n("img",{attrs:{src:a(471),width:"90%"}})]),t._v(" "),n("h3",{attrs:{id:"transformer-xl中的相对位置编码"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#transformer-xl中的相对位置编码"}},[t._v("#")]),t._v(" Transformer- XL中的相对位置编码")]),t._v(" "),n("div",{attrs:{align:"center"}},[n("img",{attrs:{src:a(472),width:"90%"}})]),t._v(" "),n("p",[t._v("实际上是输入一个query，当计算与key的相似性的时候，将p的位置向量替换为相对位置向量"),n("span",{staticClass:"katex"},[n("span",{staticClass:"katex-mathml"},[n("math",[n("semantics",[n("mrow",[n("msub",[n("mi",[t._v("R")]),n("mrow",[n("mi",[t._v("i")]),n("mo",[t._v("−")]),n("mi",[t._v("j")])],1)],1)],1),n("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("R_{i-j}")])],1)],1)],1),n("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[n("span",{staticClass:"strut",staticStyle:{height:"0.68333em"}}),n("span",{staticClass:"strut bottom",staticStyle:{height:"0.969438em","vertical-align":"-0.286108em"}}),n("span",{staticClass:"base textstyle uncramped"},[n("span",{staticClass:"mord"},[n("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.00773em"}},[t._v("R")]),n("span",{staticClass:"vlist"},[n("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"-0.00773em"}},[n("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[n("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),n("span",{staticClass:"reset-textstyle scriptstyle cramped"},[n("span",{staticClass:"mord scriptstyle cramped"},[n("span",{staticClass:"mord mathit"},[t._v("i")]),n("span",{staticClass:"mbin"},[t._v("−")]),n("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.05724em"}},[t._v("j")])])])]),n("span",{staticClass:"baseline-fix"},[n("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[n("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])])])])]),t._v("，对于query的位置向量，替换为两个可以训练的向量u、v；最终加权到v上时，直接把value的位置向量也去掉")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" torch\n\nqlen "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),t._v("               "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# query的长度，当前segment")]),t._v("\ncontext_len "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("96")]),t._v("         "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 上下文context长度，根据内存大小自定义")]),t._v("\nklen"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("224")]),t._v("                 "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# key的长度，上个segment+当前segment")]),t._v("\nd_model"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1024")]),t._v("             "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 相对位置编码维度")]),t._v("\nclamp_len"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v("             "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 将相对位置限制在某个区间")]),t._v("\nattn_type "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bi'")]),t._v("          "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 双向的")]),t._v("\nbi_data "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),t._v("           "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 双向的")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""创建相对位置编码"""')]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1. [0,2,...,1022] 长度为d_model/2=512")]),t._v("\nfreq_seq "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" d_model"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2. inv_freq的大小还是512")]),t._v("\ninv_freq "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10000")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("freq_seq "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" d_model"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ninv_freq"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape\n\nbeg"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" end "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" klen"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("qlen\nbeg"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" end\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 3. 前向和后向位置")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# [224, -127]")]),t._v("\nfwd_pos_seq "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("beg"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" end"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# [-224, 127]")]),t._v("\nbwd_pos_seq "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("beg"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("end"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1.0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 4. 定义sinusoid函数")]),t._v("\nsinusoid_inp "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("einsum"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v("'i,d->id'")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" fwd_pos_seq"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" inv_freq"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 5. 前向和后向分别编码，然后合并")]),t._v("\npos_emb "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("  torch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("concat"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("sinusoid_inp"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("sin"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" sinusoid_inp"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cos"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 6. 扩展中间维度为batchsize，每个句子都相同")]),t._v("\npos_emb "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pos_emb"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("None")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])])])}),[],!1,null,null,null);s.default=e.exports}}]);