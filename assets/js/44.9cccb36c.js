(window.webpackJsonp=window.webpackJsonp||[]).push([[44],{422:function(s,t,a){s.exports=a.p+"assets/img/Untitled.690934db.png"},423:function(s,t,a){s.exports=a.p+"assets/img/Untitled 1.7b302d62.png"},658:function(s,t,a){"use strict";a.r(t);var e=a(44),i=Object(e.a)({},(function(){var s=this,t=s.$createElement,e=s._self._c||t;return e("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[e("h1",{attrs:{id:"bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding"}},[s._v("#")]),s._v(" BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding")]),s._v(" "),e("ol",[e("li",[s._v("Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018.MLA")])]),s._v(" "),e("p",[e("em",[e("strong",[s._v("Bidirectional Encoder Representations from Transformers.")])])]),s._v(" "),e("h3",{attrs:{id:"跟之前工作的区别（gpt、elmo）"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#跟之前工作的区别（gpt、elmo）"}},[s._v("#")]),s._v(" 跟之前工作的区别（GPT、ELMo）")]),s._v(" "),e("ol",[e("li",[s._v("跟GPT的区别是，gpt考虑的是单向的，用左边的信息预测未来的信息；而bert为双向的架构，用了左侧和右侧的信息，提升某些任务的性能，提出了MLM")]),s._v(" "),e("li",[s._v("跟ELMo的区别是，elmo是RNN的架构，面对下游任务需要做调整；而bert是transformer架构，比较简单，只需要改最上层就行")])]),s._v(" "),e("p",[e("strong",[s._v("综合一下，RNN只有双向，但是架构差，GPT模型结构好，但是单向，因此BERT将这两点进行结合，得到了双向的Transformer结构")])]),s._v(" "),e("div",{attrs:{align:"center"}},[e("img",{attrs:{src:a(422),width:"100%"}})]),s._v("\n图1. bert预训练和微调\n"),e("h2",{attrs:{id:"input"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#input"}},[s._v("#")]),s._v(" INPUT")]),s._v(" "),e("div",{attrs:{align:"center"}},[e("img",{attrs:{src:a(423),width:"100%"}})]),s._v(" "),e("p",[s._v("图2. bert的输入embedding")]),s._v(" "),e("p",[s._v("相比Transformer，调了三个参数")]),s._v(" "),e("ul",[e("li",[s._v("Base：层数是L=12，维度H是768，head A是12；因此总共参数为1亿")]),s._v(" "),e("li",[s._v("Large：层数是L=24，宽度H是1024，head A是16；总共参数3.4亿")])]),s._v(" "),e("p",[e("span",{staticClass:"katex-display"},[e("span",{staticClass:"katex"},[e("span",{staticClass:"katex-mathml"},[e("math",[e("semantics",[e("mrow",[e("msub",[e("mi",[s._v("N")]),e("mrow",[e("mi",[s._v("p")]),e("mi",[s._v("a")]),e("mi",[s._v("r")]),e("mi",[s._v("a")]),e("mi",[s._v("m")]),e("mi",[s._v("s")])],1)],1),e("mo",[s._v("=")]),e("mn",[s._v("3")]),e("mn",[s._v("0")]),e("mi",[s._v("k")]),e("mo",[s._v("∗")]),e("mi",[s._v("H")]),e("mo",[s._v("+")]),e("mi",[s._v("L")]),e("mo",[s._v("∗")]),e("mo",[s._v("(")]),e("msup",[e("mi",[s._v("H")]),e("mn",[s._v("2")])],1),e("mo",[s._v("∗")]),e("mn",[s._v("8")]),e("mo",[s._v("+")]),e("msup",[e("mi",[s._v("H")]),e("mn",[s._v("2")])],1),e("mo",[s._v("∗")]),e("mn",[s._v("3")]),e("mo",[s._v("+")]),e("msup",[e("mi",[s._v("H")]),e("mn",[s._v("2")])],1),e("mo",[s._v("∗")]),e("mn",[s._v("1")]),e("mo",[s._v(")")])],1),e("annotation",{attrs:{encoding:"application/x-tex"}},[s._v("N_{params} = 30k*H + L*(H^2 * 8 + H^2 * 3 + H^2 * 1) \n")])],1)],1)],1),e("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[e("span",{staticClass:"strut",staticStyle:{height:"0.8641079999999999em"}}),e("span",{staticClass:"strut bottom",staticStyle:{height:"1.150216em","vertical-align":"-0.286108em"}}),e("span",{staticClass:"base displaystyle textstyle uncramped"},[e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.10903em"}},[s._v("N")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"-0.10903em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[s._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord scriptstyle cramped"},[e("span",{staticClass:"mord mathit"},[s._v("p")]),e("span",{staticClass:"mord mathit"},[s._v("a")]),e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.02778em"}},[s._v("r")]),e("span",{staticClass:"mord mathit"},[s._v("a")]),e("span",{staticClass:"mord mathit"},[s._v("m")]),e("span",{staticClass:"mord mathit"},[s._v("s")])])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[s._v("​")])]),s._v("​")])])]),e("span",{staticClass:"mrel"},[s._v("=")]),e("span",{staticClass:"mord mathrm"},[s._v("3")]),e("span",{staticClass:"mord mathrm"},[s._v("0")]),e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.03148em"}},[s._v("k")]),e("span",{staticClass:"mbin"},[s._v("∗")]),e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.08125em"}},[s._v("H")]),e("span",{staticClass:"mbin"},[s._v("+")]),e("span",{staticClass:"mord mathit"},[s._v("L")]),e("span",{staticClass:"mbin"},[s._v("∗")]),e("span",{staticClass:"mopen"},[s._v("(")]),e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.08125em"}},[s._v("H")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"-0.413em","margin-right":"0.05em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[s._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle uncramped"},[e("span",{staticClass:"mord mathrm"},[s._v("2")])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[s._v("​")])]),s._v("​")])])]),e("span",{staticClass:"mbin"},[s._v("∗")]),e("span",{staticClass:"mord mathrm"},[s._v("8")]),e("span",{staticClass:"mbin"},[s._v("+")]),e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.08125em"}},[s._v("H")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"-0.413em","margin-right":"0.05em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[s._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle uncramped"},[e("span",{staticClass:"mord mathrm"},[s._v("2")])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[s._v("​")])]),s._v("​")])])]),e("span",{staticClass:"mbin"},[s._v("∗")]),e("span",{staticClass:"mord mathrm"},[s._v("3")]),e("span",{staticClass:"mbin"},[s._v("+")]),e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.08125em"}},[s._v("H")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"-0.413em","margin-right":"0.05em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[s._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle uncramped"},[e("span",{staticClass:"mord mathrm"},[s._v("2")])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[s._v("​")])]),s._v("​")])])]),e("span",{staticClass:"mbin"},[s._v("∗")]),e("span",{staticClass:"mord mathrm"},[s._v("1")]),e("span",{staticClass:"mclose"},[s._v(")")])])])])])]),s._v(" "),e("ol",[e("li",[s._v("因为有些任务比如QA是看句子的关系，因此需要句子对（Sequence），这里不一定是严格的句子，可能是一个文本块，因此文中称为sequence；"),e("em",[s._v("Transformer的输入是一个句子对，因为编码器和解码器都需要输入对应句子")])]),s._v(" "),e("li",[s._v("采用了WordPiece embedding。传统方法用空格作分割单词，但如果语料很大的话，导致词典特别大，如果这个很大，就导致模型整个可学习参数都在embedding层上了；wordpiece是说如果某些词出现的概率不大的话，应该将其切开，看子序列，如果其中有词根，并且词根出现概率很大，那就只保留词根，最终3w")]),s._v(" "),e("li",[s._v("每个句子开头是[cls]，作为句子的表征，因为attention机制可以保证看到句子中的每个词，因此放在开头是ok的，另外在两个句子间放一个[sep]来区分不同句子输入，使用token type做了一个embedding")])]),s._v(" "),e("p",[s._v("最终Embedding为 "),e("strong",[s._v("token embedding")]),s._v(" + "),e("strong",[s._v("segment embedding")]),s._v(" + "),e("strong",[s._v("position embedding")])]),s._v(" "),e("h2",{attrs:{id:"pre-training"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#pre-training"}},[s._v("#")]),s._v(" Pre-training")]),s._v(" "),e("p",[s._v("（无标号的数据）")]),s._v(" "),e("h3",{attrs:{id:"task1-mlm（mask-language-model）"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#task1-mlm（mask-language-model）"}},[s._v("#")]),s._v(" Task1: MLM（Mask Language Model）")]),s._v(" "),e("ul",[e("li",[s._v("随机选择15%的词[MASK]掉")]),s._v(" "),e("li",[s._v("随机选择80%的词替换成其他词")]),s._v(" "),e("li",[s._v("随机选10%什么都不做")])]),s._v(" "),e("h3",{attrs:{id:"task2-nsp-（next-sentence-predict）"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#task2-nsp-（next-sentence-predict）"}},[s._v("#")]),s._v(" Task2: NSP （Next Sentence Predict）")]),s._v(" "),e("p",[s._v("预测上个句子是否是下个句子")]),s._v(" "),e("h2",{attrs:{id:"finetune"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#finetune"}},[s._v("#")]),s._v(" Finetune")]),s._v(" "),e("p",[s._v("（有标签的数据）")]),s._v(" "),e("p",[s._v("根据下游任务设计输入和输出，原始作者建议2-4个epoch，但是实验证明太少了，方差太大不稳定，有人建议20个epochs：*Mosbach et al even recommend fine-tunig for 20 epochs，*而且作者实验时采用的是Adam的不完全版，需要用Adam的正常版")]),s._v(" "),e("p",[e("strong",[s._v("作者用了预训练的输入做任务，跟微调的结果比差很多，因此作者建议微调使用")])])])}),[],!1,null,null,null);t.default=i.exports}}]);