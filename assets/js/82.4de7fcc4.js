(window.webpackJsonp=window.webpackJsonp||[]).push([[82],{664:function(t,r,a){"use strict";a.r(r);var e=a(44),i=Object(e.a)({},(function(){var t=this,r=t.$createElement,a=t._self._c||r;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h1",{attrs:{id:"roberta-a-robustly-optimized-bert-pretraining-approach"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#roberta-a-robustly-optimized-bert-pretraining-approach"}},[t._v("#")]),t._v(" Roberta: A robustly optimized bert pretraining approach")]),t._v(" "),a("ol",[a("li",[t._v("Liu Y, Ott M, Goyal N, et al. Roberta: A robustly optimized bert pretraining approach[J]. arXiv preprint arXiv:1907.11692, 2019.MLA\n"),a("ol",[a("li",[t._v("作者发现bert在预训练的时候，选择不同的方法和参数是很麻烦的，因此作者全面地测试了各种预训练任务的参数和任务，相当于一个bert预训练精调方案")]),t._v(" "),a("li",[t._v("作者通过一系列消融实验，得到了以下结论\n"),a("ol",[a("li",[t._v("在更大的数据集、跟大的batch和训练时间上训练，效果会一直提高")]),t._v(" "),a("li",[t._v("移除了NSP任务，发现对效果没啥影响")]),t._v(" "),a("li",[t._v("在更长的序列上训练更好")]),t._v(" "),a("li",[t._v("使用了动态掩码机制，bert是在训练前离线掩码，因此是固定的；roberta是将训练数据x10，进行10次不同的mask")])])])])])])])}),[],!1,null,null,null);r.default=i.exports}}]);