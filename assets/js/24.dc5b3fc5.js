(window.webpackJsonp=window.webpackJsonp||[]).push([[24],{461:function(t,s,a){t.exports=a.p+"assets/img/Untitled.41cf3694.png"},462:function(t,s,a){t.exports=a.p+"assets/img/Untitled 1.a4314e61.png"},463:function(t,s,a){t.exports=a.p+"assets/img/Untitled 2.e2e87ae9.png"},464:function(t,s,a){t.exports=a.p+"assets/img/Untitled 3.0042ac92.png"},670:function(t,s,a){"use strict";a.r(s);var e=a(44),i=Object(e.a)({},(function(){var t=this,s=t.$createElement,e=t._self._c||s;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h1",{attrs:{id:"transformer-attention-is-all-you-need"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#transformer-attention-is-all-you-need"}},[t._v("#")]),t._v(" Transformer:  Attention is all you need")]),t._v(" "),e("ol",[e("li",[e("strong",[t._v('Vaswani, Ashish, et al. "Attention is all you need." '),e("em",[t._v("Advances in neural information processing systems")]),t._v(" 30 (2017).APA")]),t._v(" "),e("ol",[e("li",[t._v("提出了个简单的结构，仅仅依赖"),e("strong",[t._v("注意力机制")]),t._v("，而没有用循环（RNN）和卷积（CONV）")]),t._v(" "),e("li",[t._v("RNN系列的缺点：\n"),e("ol",[e("li",[e("strong",[t._v("结构无法并行")]),t._v("，下一个词需要依赖上一个词的结果，计算性能差；")]),t._v(" "),e("li",[t._v("历史信息需要一步步往后传，如果时序比较长，"),e("strong",[t._v("早期的信息可能在后面丢失")]),t._v("，如果不想丢失，那么每一步的结果都要保存，占用很多资源")])])]),t._v(" "),e("li",[t._v("其他工作，比如用卷积神经网络替换RNN，**卷积神经网络对比较长的序列难以建模，**但是本文的Transformer可以一次看到所有信息；"),e("strong",[t._v("但是卷积网络可以有多个输出通道，每个通道可以识别不同模式，因此Transformer中加了MultiHead-Attention的机制")])]),t._v(" "),e("li",[t._v("在本文工作之前，Attention是跟 RNN一起用的，主要用在如何把编码器的东西传给解码器；"),e("strong",[t._v("本文工作是只用Attention")]),t._v("，因此就可以并行计算，而且效果很好")])])])]),t._v(" "),e("div",{attrs:{align:"center"}},[e("img",{attrs:{src:a(461),width:"70%"}})]),t._v(" "),e("h2",{attrs:{id:"embedding"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#embedding"}},[t._v("#")]),t._v(" Embedding")]),t._v(" "),e("h3",{attrs:{id:"embedding-2"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#embedding-2"}},[t._v("#")]),t._v(" Embedding")]),t._v(" "),e("p",[t._v("编码和解码的embedding是一样的，把权重乘了根号"),e("span",{staticClass:"katex"},[e("span",{staticClass:"katex-mathml"},[e("math",[e("semantics",[e("mrow",[e("msub",[e("mi",[t._v("d")]),e("mi",[t._v("k")])],1)],1),e("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("d_k")])],1)],1)],1),e("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[e("span",{staticClass:"strut",staticStyle:{height:"0.69444em"}}),e("span",{staticClass:"strut bottom",staticStyle:{height:"0.84444em","vertical-align":"-0.15em"}}),e("span",{staticClass:"base textstyle uncramped"},[e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit"},[t._v("d")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"0em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.03148em"}},[t._v("k")])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])])])])]),t._v("，因为在学embedding的时候，可能将embedding的L2long学的比较小，但后面会加上position embedding，因此把两个放到近似相同的量级上（都是-1到1之间）")]),t._v(" "),e("h3",{attrs:{id:"position-embedding"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#position-embedding"}},[t._v("#")]),t._v(" Position Embedding")]),t._v(" "),e("p",[t._v("Attention是没有序列的位置信息的，因此在embedding这里加进去；使用不同周期的cos函数来表征的，因此在-1和1之前抖动")]),t._v(" "),e("p",[t._v("(对比"),e("em",[t._v("RNN是上个时刻的输出作为下个时刻的输入，因此本身就是时序的)")])]),t._v(" "),e("h2",{attrs:{id:"encoder"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#encoder"}},[t._v("#")]),t._v(" Encoder")]),t._v(" "),e("h3",{attrs:{id:"基本架构"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#基本架构"}},[t._v("#")]),t._v(" 基本架构")]),t._v(" "),e("p",[t._v("Encoder部分包含6个Layer，每个Layer由2个sub-Layer，每个sub-Layer包含一个MultiHead-Attention和一个point-wised feed forward network，每个sub-Layer后都有先接一个残差，然后LayerNorm，因此每个sub-Layer最终为："),e("span",{staticClass:"katex"},[e("span",{staticClass:"katex-mathml"},[e("math",[e("semantics",[e("mrow",[e("mi",[t._v("L")]),e("mi",[t._v("a")]),e("mi",[t._v("y")]),e("mi",[t._v("e")]),e("mi",[t._v("r")]),e("mi",[t._v("N")]),e("mi",[t._v("o")]),e("mi",[t._v("r")]),e("mi",[t._v("m")]),e("mo",[t._v("(")]),e("mi",[t._v("x")]),e("mo",[t._v("+")]),e("mi",[t._v("S")]),e("mi",[t._v("u")]),e("mi",[t._v("b")]),e("mi",[t._v("l")]),e("mi",[t._v("a")]),e("mi",[t._v("y")]),e("mi",[t._v("e")]),e("mi",[t._v("r")]),e("mo",[t._v("(")]),e("mi",[t._v("x")]),e("mo",[t._v(")")]),e("mo",[t._v(")")])],1),e("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("LayerNorm(x+Sublayer(x))")])],1)],1)],1),e("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[e("span",{staticClass:"strut",staticStyle:{height:"0.75em"}}),e("span",{staticClass:"strut bottom",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),e("span",{staticClass:"base textstyle uncramped"},[e("span",{staticClass:"mord mathit"},[t._v("L")]),e("span",{staticClass:"mord mathit"},[t._v("a")]),e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.03588em"}},[t._v("y")]),e("span",{staticClass:"mord mathit"},[t._v("e")]),e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.02778em"}},[t._v("r")]),e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.10903em"}},[t._v("N")]),e("span",{staticClass:"mord mathit"},[t._v("o")]),e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.02778em"}},[t._v("r")]),e("span",{staticClass:"mord mathit"},[t._v("m")]),e("span",{staticClass:"mopen"},[t._v("(")]),e("span",{staticClass:"mord mathit"},[t._v("x")]),e("span",{staticClass:"mbin"},[t._v("+")]),e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.05764em"}},[t._v("S")]),e("span",{staticClass:"mord mathit"},[t._v("u")]),e("span",{staticClass:"mord mathit"},[t._v("b")]),e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.01968em"}},[t._v("l")]),e("span",{staticClass:"mord mathit"},[t._v("a")]),e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.03588em"}},[t._v("y")]),e("span",{staticClass:"mord mathit"},[t._v("e")]),e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.02778em"}},[t._v("r")]),e("span",{staticClass:"mopen"},[t._v("(")]),e("span",{staticClass:"mord mathit"},[t._v("x")]),e("span",{staticClass:"mclose"},[t._v(")")]),e("span",{staticClass:"mclose"},[t._v(")")])])])])]),t._v(" "),e("ul",[e("li",[t._v("Encoder = 6 * Layers\n"),e("ul",[e("li",[t._v("Layer = Multihead-Attention + FFN")])])])]),t._v(" "),e("div",{attrs:{align:"center"}},[e("img",{attrs:{src:a(462),width:"80%"}})]),t._v(" "),e("p",[t._v("图2. scaled Dot-Product Attention 和 Multi-head-Attention")]),t._v(" "),e("h3",{attrs:{id:"scaled-dot-product-attention"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#scaled-dot-product-attention"}},[t._v("#")]),t._v(" "),e("strong",[t._v("Scaled Dot-Product Attention")])]),t._v(" "),e("p",[t._v("Attention是计算一个query和key的相似度来得到attention score；之前的工作有不同Attention，比如add-attention和dot-product-attention，作者这里的改进是做了一个放缩，将Q*K的结果用根号"),e("span",{staticClass:"katex"},[e("span",{staticClass:"katex-mathml"},[e("math",[e("semantics",[e("mrow",[e("msub",[e("mi",[t._v("d")]),e("mi",[t._v("k")])],1)],1),e("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("d_k")])],1)],1)],1),e("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[e("span",{staticClass:"strut",staticStyle:{height:"0.69444em"}}),e("span",{staticClass:"strut bottom",staticStyle:{height:"0.84444em","vertical-align":"-0.15em"}}),e("span",{staticClass:"base textstyle uncramped"},[e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit"},[t._v("d")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"0em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.03148em"}},[t._v("k")])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])])])])]),t._v("进行缩放，因此这里作者取名"),e("strong",[t._v("Scaled Dot-Product Attention")]),t._v("，缩放的原因是因为当"),e("span",{staticClass:"katex"},[e("span",{staticClass:"katex-mathml"},[e("math",[e("semantics",[e("mrow",[e("msub",[e("mi",[t._v("d")]),e("mi",[t._v("k")])],1)],1),e("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("d_k")])],1)],1)],1),e("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[e("span",{staticClass:"strut",staticStyle:{height:"0.69444em"}}),e("span",{staticClass:"strut bottom",staticStyle:{height:"0.84444em","vertical-align":"-0.15em"}}),e("span",{staticClass:"base textstyle uncramped"},[e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit"},[t._v("d")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"0em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.03148em"}},[t._v("k")])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])])])])]),t._v("过大时，即向量长度更长，点积后的结果可能很大和很小，因此Softmax最大的值就更靠近1，其他值更靠近0，此时梯度比较小，会很难训练，因此缩放是个比较好的方式")]),t._v(" "),e("div",{attrs:{align:"center"}},[e("img",{attrs:{src:a(463),width:"70%"}})]),t._v("\n    图3. Attention矩阵相乘的过程\n"),e("h3",{attrs:{id:"masked-multi-head-attention"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#masked-multi-head-attention"}},[t._v("#")]),t._v(" Masked Multi-head Attention")]),t._v(" "),e("p",[t._v("Masked Multi-head Attention中的"),e("strong",[t._v("Mask")]),t._v("是因为在计算t时刻的attention，"),e("span",{staticClass:"katex"},[e("span",{staticClass:"katex-mathml"},[e("math",[e("semantics",[e("mrow",[e("msub",[e("mi",[t._v("q")]),e("mi",[t._v("t")])],1)],1),e("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("q_t")])],1)],1)],1),e("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[e("span",{staticClass:"strut",staticStyle:{height:"0.43056em"}}),e("span",{staticClass:"strut bottom",staticStyle:{height:"0.625em","vertical-align":"-0.19444em"}}),e("span",{staticClass:"base textstyle uncramped"},[e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.03588em"}},[t._v("q")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"-0.03588em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord mathit"},[t._v("t")])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])])])])]),t._v("时，此时只能看到前k-1时刻的情况，理论上是不知道后面时刻的值的，因此将后面的值Mask掉，具体就是取一个很负的值，此时softmax出来的值对应的权重都为0，只有k-1时刻和之前时刻的值；Multi-head是对于单个head的attention，不如"),e("strong",[t._v("将其投影到多个比较低的维度上去")]),t._v("，对于每个维度的输出，在最终并到一起，再投影会来得到最终的输出；")]),t._v(" "),e("p",[e("strong",[t._v("为什么用Multi-head Attention？")]),t._v(" 是因为dot-product本身是没什么参数可以学的，就是两个点积，但是这里的linear的权重w是可以学习的，网络内部通过不同方式去处理；也就是给你h次机会，使得在投影进去的度量空间里匹配不同模式")]),t._v(" "),e("h3",{attrs:{id:"self-attention"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#self-attention"}},[t._v("#")]),t._v(" Self-Attention")]),t._v(" "),e("p",[t._v("Self-Attention：这里就是q、k、v都是自己，QK就相当计算每个token的相似度作为score，最终加权到v上去，这里肯定是自己跟自己的score最大，但是其他词对当前词也会有影响，而且分成多个head就投影到多个空间上去，肯定可以学到不同空间的影响")]),t._v(" "),e("h3",{attrs:{id:"point-wise-feed-forward-network"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#point-wise-feed-forward-network"}},[t._v("#")]),t._v(" Point-wise Feed Forward Network")]),t._v(" "),e("p",[t._v("作者将一句话中的每个词看作一个个点（point），因此名字叫point-wise；实际上就是个MLP，单个隐藏层，把输入从512扩大到2048，然后再缩放回去，这里的max就是激活函数Relu")]),t._v(" "),e("p",[e("span",{staticClass:"katex-display"},[e("span",{staticClass:"katex"},[e("span",{staticClass:"katex-mathml"},[e("math",[e("semantics",[e("mrow",[e("mi",[t._v("F")]),e("mi",[t._v("F")]),e("mi",[t._v("N")]),e("mo",[t._v("(")]),e("mi",[t._v("x")]),e("mo",[t._v(")")]),e("mo",[t._v("=")]),e("mi",[t._v("m")]),e("mi",[t._v("a")]),e("mi",[t._v("x")]),e("mo",[t._v("(")]),e("mn",[t._v("0")]),e("mo",{attrs:{separator:"true"}},[t._v(",")]),e("mi",[t._v("x")]),e("msub",[e("mi",[t._v("W")]),e("mn",[t._v("1")])],1),e("mo",[t._v("+")]),e("msub",[e("mi",[t._v("b")]),e("mn",[t._v("1")])],1),e("mo",[t._v(")")]),e("msub",[e("mi",[t._v("W")]),e("mn",[t._v("2")])],1),e("mo",[t._v("+")]),e("msub",[e("mi",[t._v("b")]),e("mn",[t._v("2")])],1)],1),e("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("FFN(x) = max(0, xW_1+b_1)W_2+b_2\n")])],1)],1)],1),e("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[e("span",{staticClass:"strut",staticStyle:{height:"0.75em"}}),e("span",{staticClass:"strut bottom",staticStyle:{height:"1em","vertical-align":"-0.25em"}}),e("span",{staticClass:"base displaystyle textstyle uncramped"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.13889em"}},[t._v("F")]),e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.13889em"}},[t._v("F")]),e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.10903em"}},[t._v("N")]),e("span",{staticClass:"mopen"},[t._v("(")]),e("span",{staticClass:"mord mathit"},[t._v("x")]),e("span",{staticClass:"mclose"},[t._v(")")]),e("span",{staticClass:"mrel"},[t._v("=")]),e("span",{staticClass:"mord mathit"},[t._v("m")]),e("span",{staticClass:"mord mathit"},[t._v("a")]),e("span",{staticClass:"mord mathit"},[t._v("x")]),e("span",{staticClass:"mopen"},[t._v("(")]),e("span",{staticClass:"mord mathrm"},[t._v("0")]),e("span",{staticClass:"mpunct"},[t._v(",")]),e("span",{staticClass:"mord mathit"},[t._v("x")]),e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.13889em"}},[t._v("W")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"-0.13889em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord mathrm"},[t._v("1")])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])]),e("span",{staticClass:"mbin"},[t._v("+")]),e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit"},[t._v("b")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"0em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord mathrm"},[t._v("1")])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])]),e("span",{staticClass:"mclose"},[t._v(")")]),e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.13889em"}},[t._v("W")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"-0.13889em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord mathrm"},[t._v("2")])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])]),e("span",{staticClass:"mbin"},[t._v("+")]),e("span",{staticClass:"mord"},[e("span",{staticClass:"mord mathit"},[t._v("b")]),e("span",{staticClass:"vlist"},[e("span",{staticStyle:{top:"0.15em","margin-right":"0.05em","margin-left":"0em"}},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),e("span",{staticClass:"reset-textstyle scriptstyle cramped"},[e("span",{staticClass:"mord mathrm"},[t._v("2")])])]),e("span",{staticClass:"baseline-fix"},[e("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[e("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])])])])])])]),t._v(" "),e("p",[t._v("这里的作用就是前面Attention已经对于每个token都把这个序列信息进行了一个抽取，然后这里通过对每个token的序列信息进行进一步加工，然后再输出")]),t._v(" "),e("p",[e("em",[e("strong",[t._v("这里跟RNN的区别就是Transformer是通过attention将一句话完整的序列信息整合到每个token上，而rnn是每次将上一时刻的信息传给下个时刻，然后跟当前时刻的词一起输入；因此序列信息的整合方式不同，两者都用了MLP做最终的信息整合")])])]),t._v(" "),e("h2",{attrs:{id:"decoder"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#decoder"}},[t._v("#")]),t._v(" Decoder")]),t._v(" "),e("p",[t._v("Decoder部分也是包括6个Layer，除了Encoder中相同的2个sub-Layer，在这之前，还插入了第三个sub-Layer：Masked-Multihead-Attention模块；同样每个sub-Layer后都有先接一个残差，然后LayerNorm；")]),t._v(" "),e("ul",[e("li",[t._v("Decoder = 6 * Layers\n"),e("ul",[e("li",[t._v("Layer =Masked-Multihead-Attention +  Multihead-Attention + FFN")])])])]),t._v(" "),e("h2",{attrs:{id:"transformer的三种attention"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#transformer的三种attention"}},[t._v("#")]),t._v(" Transformer的三种Attention")]),t._v(" "),e("div",{attrs:{align:"center"}},[e("img",{attrs:{src:a(464),width:"80%"}})]),t._v(" "),e("p",[t._v("图4. Transformer中的3种Attention")]),t._v(" "),e("p",[t._v("整个Transformer有三种Attention")]),t._v(" "),e("ul",[e("li",[t._v("encoder中的self-attention")]),t._v(" "),e("li",[t._v("decoder中的masked self-attention")]),t._v(" "),e("li",[t._v("decoder中解码时的attention\n"),e("ul",[e("li",[t._v("query为mask-attention的输出，key、value为encoder的输出，相当于看query和训练到的key的相似度，得到attention score，最终作用在学习到的value上，产生最终的输出")])])])]),t._v(" "),e("h2",{attrs:{id:"关于模型"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#关于模型"}},[t._v("#")]),t._v(" 关于模型")]),t._v(" "),e("ul",[e("li",[t._v("题目名字为“Attention is All you need”，实际上不只是Attention，少了MLP、残差、position都训不出东西来")]),t._v(" "),e("li",[t._v("原本图像用cnn，文本用rnn，这个模型统一都用attention提取特征了，某种程度上的统一")]),t._v(" "),e("li",[t._v("attention的基本假设太简单了，因此对数据信息的抓取能力变差了，因此需要多层，多数据，导致现在的模型都必须很大，数据需要很多")])])])}),[],!1,null,null,null);s.default=i.exports}}]);