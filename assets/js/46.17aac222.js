(window.webpackJsonp=window.webpackJsonp||[]).push([[46],{424:function(t,e,n){t.exports=n.p+"assets/img/Untitled.82401973.png"},425:function(t,e,n){t.exports=n.p+"assets/img/Untitled 1.d415ddb8.png"},660:function(t,e,n){"use strict";n.r(e);var i=n(44),a=Object(i.a)({},(function(){var t=this,e=t.$createElement,i=t._self._c||e;return i("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[i("h1",{attrs:{id:"deberta-decoding-enhanced-bert-with-disentangled-attention"}},[i("a",{staticClass:"header-anchor",attrs:{href:"#deberta-decoding-enhanced-bert-with-disentangled-attention"}},[t._v("#")]),t._v(" Deberta: Decoding-enhanced bert with disentangled attention")]),t._v(" "),i("div",{attrs:{align:"center"}},[i("img",{attrs:{src:n(424),width:"100%"}})]),t._v("\n图1. deberta架构\n"),i("ol",[i("li",[t._v("He P, Liu X, Gao J, et al. Deberta: Decoding-enhanced bert with disentangled attention[J]. arXiv preprint arXiv:2006.03654, 2020.MLA\n"),i("ol",[i("li",[t._v("提出了一种全新的bert架构，deberta，使用了两种技术超越bert和roberta\n"),i("ol",[i("li",[t._v("第一是分散注意力attention机制，每个词使用两个向量分别代表内容和相对位置，并且词之间的attention score在内容和相对位置上分别计算；因为观察词对的attention权重不仅依赖于内容还依赖于相对位置；因此在计算attention的时候，计算了content-content、content-position、position-content三部分，本来第四部分position-position没什么用，就不要了")]),t._v(" "),i("li",[t._v("第二是使用一个enhanced mask decoder，将绝对位置编码在解码层进行合并，从而在与预练中预测mask word；因为虽然encoder部分使用了相对位置信息，但语言模型中句子的绝对位置也很重要，有时绝对位置包含了主语宾语等信息，因此在softmax前将绝对位置编码合并；在softmax层基于内容和位置进行掩码；")]),t._v(" "),i("li",[t._v("此外，采用了尺度不变finetune的虚拟对抗训练的方法去微调，从而提高泛化性；具体是之前的对抗训练在embedding上加扰动，但是经过很多层后，扰动可能被放大很多，影响学习，因此作者在微调前，对embedding进行标准化，然后在这个embedding上加扰动，可以限定此时句向量的方差不会太大")])])])])])]),t._v(" "),i("div",{attrs:{align:"center"}},[i("img",{attrs:{src:n(425),width:"80%"}})]),t._v(" "),i("p",[t._v("图2. EMD结构")])])}),[],!1,null,null,null);e.default=a.exports}}]);