(window.webpackJsonp=window.webpackJsonp||[]).push([[53],{378:function(t,a,s){t.exports=s.p+"assets/img/adda.0b2aef60.png"},639:function(t,a,s){"use strict";s.r(a);var n=s(44),e=Object(n.a)({},(function(){var t=this,a=t.$createElement,n=t._self._c||a;return n("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[n("h1",{attrs:{id:"adversarial-discriminative-domain-adaptation"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#adversarial-discriminative-domain-adaptation"}},[t._v("#")]),t._v(" Adversarial Discriminative Domain Adaptation")]),t._v(" "),n("h2",{attrs:{id:"理论基础"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#理论基础"}},[t._v("#")]),t._v(" 理论基础")]),t._v(" "),n("p",[t._v("深度学习训练的一个假设是训练集和测试集是"),n("strong",[t._v("相同分布")]),t._v("的，因此模型经过训练集训练后，往往在测试集上也会达到比较好的效果（比如将训练集按比例划分为训练、测试部分）；但是实际中，测试集和训练集往往是不同分布的，因此在测试集上的效果会大打折扣；")]),t._v(" "),n("p",[t._v("理想的情况下，feature extractor提取的特征应该是高度抽象并且固定的，而后续的classifier负责根据feature判断目标类别，此时feature extractor和classifer应该是更通用的，而不随训练数据和测试数据分布不同产生改变，在训练数据训练后的model，可以直接应用在目标数据上，获得近似相同的效果")]),t._v(" "),n("p",[t._v("域自适应（domain adaption）就是为了解决这种问题，其核心思想为：将source和target的数据映射到相同分布的空间上，这样两者的数据差异就会减小，从而当成同一个数据集")]),t._v(" "),n("h2",{attrs:{id:"域自适应对抗训练"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#域自适应对抗训练"}},[t._v("#")]),t._v(" 域自适应对抗训练")]),t._v(" "),n("p",[t._v("域自适应对抗训练就是一种将source和target的数据分布拉近的方式，主要包括以下几个步骤")]),t._v(" "),n("ol",[n("li",[t._v("定义源域和目标域，分别有各自的encoder进行特征提取，分别有各自的classifer")]),t._v(" "),n("li",[t._v("目标为：将各自encoder提取的特征尽可能接近，从而source学习得到的classifer可以直接作用于target上")]),t._v(" "),n("li",[t._v("由于源域数据存在label，因此首先训练source encoder和classifier")]),t._v(" "),n("li",[t._v("通过对抗的方法，同时输入source和target数据，训练作用于目标域的encoder和一个classifier（discriminator，用于判别输入来自source还是target，将判别器效果达到最优）")]),t._v(" "),n("li",[t._v("判别器达到最优后，按住其不动，输入target数据训练encoder，让判别器尽量失误，此时target encoder逐渐趋于source encoder")]),t._v(" "),n("li",[t._v("使用目标域的encoder和源域的classifer进行最终类别的判定")])]),t._v(" "),n("div",{attrs:{align:"center"}},[n("img",{attrs:{src:s(378),width:"100%"}})]),t._v(" "),n("p",[t._v("其中，虚线框代表参数固定，实线框代表参数学习")]),t._v(" "),n("div",{staticClass:"language-python extra-class"},[n("pre",{pre:!0,attrs:{class:"language-python"}},[n("code",[n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1. pre-training")]),t._v("\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 正常根据source data训练模型")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2. adversaial adaption")]),t._v("\n\n"),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token function"}},[t._v("train_tgt")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("src_encoder"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" tgt_encoder"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" discriminator"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n              src_data_loader"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" tgt_data_loader"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""Train encoder for target domain."""')]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("########################################")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 1. 初始化tgt_encoder 和 discriminator #")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("########################################")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# set train state for Dropout and BN layers")]),t._v("\n    tgt_encoder"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("   "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 最初使用src_encoder的参数初始化tgt_encoder")]),t._v("\n    discriminator"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("train"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# setup criterion and optimizer")]),t._v("\n    criterion "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" nn"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("CrossEntropyLoss"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# discriminator的loss")]),t._v("\n    optimizer_tgt "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" optim"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Adam"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tgt_encoder"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parameters"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("lr"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("betas"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.9")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    optimizer_discriminator "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" optim"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Adam"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("discriminator"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parameters"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("lr"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e")]),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("betas"),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.9")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    len_data_loader "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("min")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("src_data_loader"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tgt_data_loader"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("################################")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2. 对抗训练 && 训练tgt encoder #")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("################################")]),t._v("\n\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" epoch "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# zip source and target data pair")]),t._v("\n        data_zip "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("zip")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("src_data_loader"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" tgt_data_loader"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" step"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("images_src"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" _"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("images_tgt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" _"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" data_zip"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("##########################")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#  2.1 训练discriminator  #")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("##########################")]),t._v("\n\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# make images variable")]),t._v("\n            images_src "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" make_variable"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("images_src"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            images_tgt "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" make_variable"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("images_tgt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# zero gradients for optimizer")]),t._v("\n            optimizer_discriminator"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zero_grad"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 将不同encoder提取的feature混合，一起输入discriminator，使其达到最优")]),t._v("\n            feat_src "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" src_encoder"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("images_src"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            feat_tgt "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tgt_encoder"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("images_tgt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            feat_concat "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cat"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("feat_src"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" feat_tgt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# predict on discriminator")]),t._v("\n            pred_concat "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" discriminator"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("feat_concat"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("detach"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# prepare real and fake label")]),t._v("\n            label_src "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" make_variable"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("torch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ones"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("feat_src"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("long")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            label_tgt "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" make_variable"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("torch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zeros"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("feat_tgt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("long")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            label_concat "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cat"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("label_src"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" label_tgt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# compute loss for discriminator")]),t._v("\n            loss_discriminator "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" criterion"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pred_concat"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" label_concat"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            loss_discriminator"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("backward"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# optimize critic")]),t._v("\n            optimizer_discriminator"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("step"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\t\t\t\t\t\t"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# acc越高，代表discriminator效果更好")]),t._v("\n            pred_cls "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("squeeze"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pred_concat"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("max")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            acc "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pred_cls "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" label_concat"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("float")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("mean"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \n\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("##############################")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2.2 对target encoder进行训练 #")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("##############################")]),t._v("\n\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# zero gradients for optimizer")]),t._v("\n            optimizer_discriminator"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zero_grad"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            optimizer_tgt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("zero_grad"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# extract and target features")]),t._v("\n            feat_tgt "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" tgt_encoder"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("images_tgt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# predict on discriminator 单独输入tgt的特征")]),t._v("\n            pred_tgt "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" discriminator"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("feat_tgt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# prepare fake labels ")]),t._v("\n\t\t\t\t\t\t"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 与一起输入时的label相反，对抗训练")]),t._v("\n            label_tgt "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" make_variable"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("torch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ones"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("feat_tgt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("long")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# compute loss for target encoder")]),t._v("\n\t\t\t\t\t\t"),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 目标是让discriminator判断失误，从而训练tgt encoder")]),t._v("\n            loss_tgt "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" criterion"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("pred_tgt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" label_tgt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            loss_tgt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("backward"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# optimize target encoder")]),t._v("\n            optimizer_tgt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("step"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#######################")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2.3   打印结果       #")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#######################")]),t._v("\n            "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("step "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" params"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("log_step "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Epoch [{}/{}] Step [{}/{}]:"')]),t._v("\n                      "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"d_loss={:.5f} g_loss={:.5f} acc={:.5f}"')]),t._v("\n                      "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("epoch "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                              params"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("num_epochs"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                              step "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                              len_data_loader"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                              loss_discriminator"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("item"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                              loss_tgt"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("item"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                              acc"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("item"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#############################")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 2.4 save model parameters #")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token comment"}},[t._v("#############################")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("epoch "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" params"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("save_step "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            torch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("save"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("discriminator"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("state_dict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" os"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n                params"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model_root"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"ADDA-discriminator-{}.pt"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("epoch "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            torch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("save"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tgt_encoder"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("state_dict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" os"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n                params"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model_root"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"ADDA-target-encoder-{}.pt"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),n("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("epoch "),n("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),n("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    torch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("save"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("discriminator"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("state_dict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" os"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        params"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model_root"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"ADDA-discriminator-final.pt"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    torch"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("save"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("tgt_encoder"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("state_dict"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" os"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("join"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        params"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model_root"),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        "),n("span",{pre:!0,attrs:{class:"token string"}},[t._v('"ADDA-target-encoder-final.pt"')]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),n("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),n("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" tgt_encoder\n")])])]),n("h3",{attrs:{id:"注意"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#注意"}},[t._v("#")]),t._v(" 注意")]),t._v(" "),n("p",[t._v("以上训练过程，代表了"),n("strong",[t._v("GAN")]),t._v("交替训练的两个过程：")]),t._v(" "),n("ol",[n("li",[t._v("拿一批真假混合的数据训练判别器discriminator，让其达到最佳，可以区分输入来源；")]),t._v(" "),n("li",[t._v("固定判别器discriminator，输入假数据，给fake label，让判别器判别错误，从而训练encoder，使其达到最优，当判别器效果最好的时候都判别错误，意味着真假数据的encoder已经相同")])]),t._v(" "),n("h2",{attrs:{id:"references"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#references"}},[t._v("#")]),t._v(" References")]),t._v(" "),n("p",[t._v("[1]. "),n("a",{attrs:{href:"https://arxiv.org/abs/1702.05464",target:"_blank",rel:"noopener noreferrer"}},[t._v("Adversarial Discriminative Domain Adaptation"),n("OutboundLink")],1)]),t._v(" "),n("p",[t._v("[2]. "),n("a",{attrs:{href:"https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf",target:"_blank",rel:"noopener noreferrer"}},[t._v("Generative Adversarial Nets"),n("OutboundLink")],1)])])}),[],!1,null,null,null);a.default=e.exports}}]);